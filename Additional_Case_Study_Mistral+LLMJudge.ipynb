{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sW4FpPGRQhmx"
      },
      "source": [
        "<center><img src=\"https://upload.wikimedia.org/wikipedia/commons/e/e9/4_RGB_McCombs_School_Brand_Branded.png\" width=\"300\" height=\"100\"/>\n",
        "  <img src=\"https://mma.prnewswire.com/media/1458111/Great_Learning_Logo.jpg?p=facebook\" width=\"200\" height=\"100\"/></center>\n",
        "\n",
        "<center><font size=10>Generative AI for Business Applications</center></font>\n",
        "<center><font size=6>Retrieval Augmented Generation - Week 3</center></font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1EgG25UQ3PX"
      },
      "source": [
        "<center><img src=\"https://i.ibb.co/pBF9nKpf/apple.png\" width=\"720\"></center>\n",
        "\n",
        "<center><font size=6>Apple HBR Report Document Q&A</center></font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgLfhM7hStpK"
      },
      "source": [
        "# Problem Statement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49Hj1OD_kkWG"
      },
      "source": [
        "## Business Context"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pegcATWWStpK"
      },
      "source": [
        "As organizations grow and scale, they are often inundated with large volumes of data, reports, and documents that contain critical information for decision-making. In real-world business settings, such as venture capital firms like Andreesen Horowitz, business analysts are required to sift through large datasets, research papers, or reports to extract relevant information that impacts strategic decisions.\n",
        "\n",
        "For instance, consider that you've just joined Andreesen Horowitz, a renowned venture capital firm, and you are tasked with analyzing a dense report like the Harvard Business Review's **\"How Apple is Organized for Innovation.\"** Going through the report manually can be extremely time-consuming as the size and complexity of these report increases. However, by using **Semantic Search** and **Retrieval-Augmented Generation (RAG)** models, you can significantly streamline this process.\n",
        "\n",
        "Imagine having the capability to directly ask questions like, “How does Apple structure its teams for innovation?” and get immediate, relevant answers drawn from the report. This ability to extract and organize specific insights quickly and accurately enables you to focus on higher-level analysis and decision-making, rather than being bogged down by information retrieval."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYKE35BRkspB"
      },
      "source": [
        "## Objective"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcksbDgdlHhA"
      },
      "source": [
        "The goal is to develop a RAG application that helps business analysts efficiently extract key insights from extensive reports, such as “How Apple is Organized for Innovation.”\n",
        "\n",
        "Specifically, the system aims to:\n",
        "\n",
        "- Answer user queries by retrieving relevant content directly from lengthy documents.\n",
        "\n",
        "- Support natural-language interaction without requiring a full manual read-through.\n",
        "\n",
        "- Act as an intelligent assistant that streamlines the report analysis process.\n",
        "\n",
        "Through this solution, analysts can save time, improve productivity, and make faster, more informed strategic decisions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3f5p5bLolH7f"
      },
      "source": [
        "## Data Description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KagUxRh3lKiv"
      },
      "source": [
        "**How Apple is Organized for Innovation** - An article of 11 pages in pdf format"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgAFFFruPmpH"
      },
      "source": [
        "# Installing and Importing the Necessary Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z4jB1V4bmViq"
      },
      "outputs": [],
      "source": [
        "# Installation for GPU llama-cpp-python\n",
        "# uncomment and run the following code in case GPU is being used\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python==0.1.85 --force-reinstall --no-cache-dir -q\n",
        "\n",
        "# Installation for CPU llama-cpp-python\n",
        "# uncomment and run the following code in case GPU is not being used\n",
        "# !CMAKE_ARGS=\"-DLLAMA_CUBLAS=off\" FORCE_CMAKE=1 pip install llama-cpp-python==0.1.85 --force-reinstall --no-cache-dir -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maerjDCGnxsh"
      },
      "source": [
        "**Note**:\n",
        "- After running the above cell, kindly restart the runtime (for Google Colab) or notebook kernel (for Jupyter Notebook), and run all cells sequentially from the next cell.\n",
        "- On executing the above line of code, you might see a warning regarding package dependencies. This error message can be ignored as the above code ensures that all necessary libraries and their dependencies are maintained to successfully execute the code in ***this notebook***."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For installing the libraries & downloading models from HF Hub\n",
        "!pip install -q pandas \\\n",
        "            tiktoken \\\n",
        "            pymupdf \\\n",
        "            langchain \\\n",
        "            langchain-community \\\n",
        "            chromadb \\\n",
        "            sentence-transformers \\\n",
        "            datasets"
      ],
      "metadata": {
        "id": "QtUw1Gq7ZaXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CG4B9XCIpEy3"
      },
      "source": [
        "**Note**:\n",
        "- After running the above cell, kindly restart the runtime (for Google Colab) or notebook kernel (for Jupyter Notebook), and run all cells sequentially from the next cell.\n",
        "- On executing the above line of code, you might see a warning regarding package dependencies. This error message can be ignored as the above code ensures that all necessary libraries and their dependencies are maintained to successfully execute the code in ***this notebook***."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HKMjqQpFl4kd"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import tiktoken\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "#Libraries for Loading Data, Chunking, Embedding, and Vector Databases\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import PyMuPDFLoader\n",
        "from langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "\n",
        "from huggingface_hub import hf_hub_download\n",
        "from llama_cpp import Llama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KVNQv8HasUni"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVUcJXbLYYp0"
      },
      "source": [
        "# Loading the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b3wGwCZjo3Zh"
      },
      "outputs": [],
      "source": [
        "# uncomment and run the below code snippets if the dataset is present in the Google Drive\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FLkk3OF1YXVJ"
      },
      "outputs": [],
      "source": [
        "pdf_file = \"/content/HBR_How_Apple_Is_Organized_For_Innovation.pdf\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KltKUFH5YXVK"
      },
      "outputs": [],
      "source": [
        "pdf_loader = PyMuPDFLoader(pdf_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUl-PAEabKLm"
      },
      "source": [
        "### Downloading and loading the LLM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlIP3ecRmHmS"
      },
      "source": [
        "We are going to download and use the Llama model which is trained on 13 billion parameters. The size of this model is around 9GB so it is recommended to have a good internet connection along with a GPU to download it and generate responses from it respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d-qLeKZZb7LM"
      },
      "outputs": [],
      "source": [
        "model_name_or_path = \"TheBloke/Mistral-7B-Instruct-v0.1-GGUF\"\n",
        "model_basename = \"mistral-7b-instruct-v0.1.Q2_K.gguf\" # the model is in gguf format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tH9ID6h9b7LN"
      },
      "outputs": [],
      "source": [
        "model_path = hf_hub_download(\n",
        "    repo_id=model_name_or_path,\n",
        "    filename=model_basename\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZBh6g5qb7LN"
      },
      "source": [
        "The given code downloads a model file from **Hugging Face Hub** using the `hf_hub_download` function. Here's a breakdown:\n",
        "\n",
        "- `hf_hub_download(...)`: Fetches a file from the **Hugging Face Model Hub**.\n",
        "- `repo_id=model_name_or_path`: Specifies the **repository ID** (i.e., the model's name or path on Hugging Face).\n",
        "- `filename=model_basename`: Specifies the **file name** to download from the model repository.\n",
        "\n",
        "This is typically used to **download pre-trained models**, embeddings, or other necessary files from Hugging Face for tasks like **text generation, embeddings, or fine-tuning**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "46dB9ZqQ9fhl"
      },
      "outputs": [],
      "source": [
        "#uncomment the below snippet of code if the runtime is connected to GPU.\n",
        "lcpp_llm = Llama(\n",
        "    model_path=model_path,\n",
        "    n_ctx=2300,\n",
        "    n_gpu_layers=38,\n",
        "    n_batch=512\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9FjKutIA9fhl"
      },
      "outputs": [],
      "source": [
        "# # uncomment the below snippet of code if the runtime is connected to CPU only.\n",
        "# lcpp_llm = Llama(\n",
        "#    model_path=model_path,\n",
        "#    n_ctx=2300\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyANkhehb7LO"
      },
      "source": [
        "The given code initializes a **Llama model**for local inference. Here's a breakdown of each parameter:\n",
        "\n",
        "- **`Llama(...)`**: Loads a **Llama model** for text generation.\n",
        "- **`model_path=model_path`**: Specifies the **file path** of the downloaded model (from Hugging Face or another source).\n",
        "- **`n_ctx=..`**: Sets the **context window** (i.e., the maximum number of tokens the model can process at once).\n",
        "- **`n_batch=..`**: Defines the **batch size** for processing tokens. A higher value improves speed but requires more VRAM.\n",
        "- **`n_gpu_layers=..`**: Determines how many **layers** are offloaded to the **GPU**. Adjust this based on available **VRAM**.\n",
        "\n",
        "This setup is optimized for **running a local Llama model**, leveraging both **CPU and GPU** for efficient inference. The parameters should be adjusted based on **hardware constraints** (CPU, GPU, and RAM availability)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_oE9pSIk84c"
      },
      "source": [
        "# Question Answering using Base model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generation Function"
      ],
      "metadata": {
        "id": "DQ7lR9WLlOTh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response(user_input , llm):\n",
        "\n",
        "    # Quering an LLM\n",
        "    try:\n",
        "        response = llm(\n",
        "                prompt=user_input,\n",
        "                max_tokens=512,\n",
        "                temperature=0.4,\n",
        "                top_p=0.95,\n",
        "                repeat_penalty=1.2,\n",
        "                top_k=25,\n",
        "                stop=['INST'],\n",
        "                echo=False\n",
        "                )\n",
        "\n",
        "        prediction =  response[\"choices\"][0][\"text\"]\n",
        "\n",
        "    except Exception as e:\n",
        "        prediction = f'Sorry, I encountered the following error: \\n {e}'\n",
        "\n",
        "    return  prediction"
      ],
      "metadata": {
        "id": "dUKV7N9XlSlB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yaJMLOsDk84c"
      },
      "source": [
        "### Question 1: Who are the authors of this article and who published this article ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-xswiybk84c"
      },
      "outputs": [],
      "source": [
        "question_1 = \"Who are the authors of this article and who published this article ?\"\n",
        "base_answer_1=generate_response(question_1,lcpp_llm)\n",
        "print(base_answer_1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfqbhEtSk84d"
      },
      "source": [
        "### Question 2: List down the three leadership characteristics in bulleted points and explain each one of the characteristics under two lines."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WAEDiiIFk84d"
      },
      "outputs": [],
      "source": [
        "question_2 = \"List down the three leadership characteristics in bulleted points and explain each one of the characteristics under two lines.\"\n",
        "base_answer_2=generate_response(question_2,lcpp_llm)\n",
        "base_answer_2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5_a6Hv4k84d"
      },
      "source": [
        "### Question 3: Can you explain specific examples from the article where Apple's approach to leadership has led to successful innovations?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ZHlnwzNk84e"
      },
      "outputs": [],
      "source": [
        "question_3 = \"Can you explain specific examples from the article where Apple's approach to leadership has led to successful innovations?\"\n",
        "base_answer_3=generate_response(question_3,lcpp_llm)\n",
        "base_answer_3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t63YyaeidcQ-"
      },
      "source": [
        "# Retrieval Augmented Generation Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKoXO1OYCXs2"
      },
      "source": [
        "### Split the Loaded PDF into Chunks for Further Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EJXwUPWCxM8J"
      },
      "outputs": [],
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    encoding_name='cl100k_base',\n",
        "    chunk_size=512,\n",
        "    chunk_overlap=20\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8QosusIl6F1"
      },
      "source": [
        "The given code initializes a **RecursiveCharacterTextSplitter** to split the text into manageable chunks for embedding and retrieval. Here's a breakdown:\n",
        "\n",
        "- `RecursiveCharacterTextSplitter.from_tiktoken_encoder(...)`: Uses **TikToken encoding** to properly handle token-based splitting.\n",
        "- `encoding_name='cl100k_base'`: Specifies the **TikToken encoding** (used by OpenAI models like GPT-4 and GPT-3.5).\n",
        "- `chunk_size=512`: Each text chunk will have a maximum of **512 tokens**.\n",
        "- `chunk_overlap=16`: Ensures **overlapping** of 16 tokens between consecutive chunks to preserve context.\n",
        "\n",
        "This approach ensures that text is split **intelligently** while maintaining **semantic meaning** for better retrieval and embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fp9yToSobbZu"
      },
      "outputs": [],
      "source": [
        "document_chunks = pdf_loader.load_and_split(text_splitter)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9ny5vsSVUS8"
      },
      "source": [
        "(Note: Expect that the above cell will take time to execute)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WewdBkHrD6pA"
      },
      "source": [
        "Let's take a look at consecutive chunks from the document."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vjAO9Z84EHnA"
      },
      "outputs": [],
      "source": [
        "i = 5\n",
        "document_chunks[i]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nLuUFIza5aAk"
      },
      "outputs": [],
      "source": [
        "document_chunks[i+1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rrmpizT6qrP"
      },
      "source": [
        "As we can see there is some overlap between the chunks. This improves the coherence and relevance of retrieved results, as the model can better understand the relationship between adjacent parts of the document. It also helps in maintaining the flow of ideas and ensuring that critical context is available when generating answers, leading to more accurate and contextually consistent outputs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdB5Kr5B-EsZ"
      },
      "source": [
        "### Generate Vector Embeddings for Text Chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nwusGdTRxhhP"
      },
      "outputs": [],
      "source": [
        "embedding_model = SentenceTransformerEmbeddings(model_name='thenlper/gte-large')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxVDXdV2imGE"
      },
      "source": [
        "Now that we have chunked the raw input, **we can present these chunks to an embedding model and then store the generated embeddings into a vector database.**\n",
        "  - We generate a vector for each chunk and save this chunk along with the vector representation in a specialized database."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Egn9xt_B9Muk"
      },
      "source": [
        "### Creating a Vector Database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NFbwm8l1sW1J"
      },
      "outputs": [],
      "source": [
        "out_dir = 'apple_db'\n",
        "\n",
        "if not os.path.exists(out_dir):\n",
        "  os.makedirs(out_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "972yZSXwcdpH"
      },
      "outputs": [],
      "source": [
        "vectorstore = Chroma.from_documents(\n",
        "    document_chunks,\n",
        "    embedding_model,\n",
        "    persist_directory=out_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNbhfJ3VlZzJ"
      },
      "source": [
        "The given code initializes a **vector database** (also called **vector store**) using **Chroma**, a popular open-source vector database, to store **document embeddings** for retrieval in a **Retrieval-Augmented Generation (RAG)** system. Here's a breakdown of what each part does:  \n",
        "\n",
        "- **`vectorstore = Chroma.from_documents(...)`**  \n",
        "   - This creates a **Chroma vector store** from a set of **document chunks**. Chroma is used to store and retrieve embeddings efficiently.\n",
        "\n",
        "- **Parameters Passed to `Chroma.from_documents()`**  \n",
        "   - `document_chunks`: A list of **text chunks** (split portions of a document) that will be converted into embeddings.  \n",
        "   - `embedding_model`: The model responsible for **embedding** the document chunks into vector representations. Common choices include OpenAI’s embeddings, Sentence Transformers, or other dense vector models.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DEECCFPqsex6"
      },
      "outputs": [],
      "source": [
        "vectorstore = Chroma(persist_directory=out_dir,embedding_function=embedding_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f57VodWykVxA"
      },
      "source": [
        "### Retrieval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aROMSqTJFJd2"
      },
      "source": [
        "We will now create a retriever that can query an input text and retrieve the top$-k$ documents that are most relevant from the vector store.\n",
        "\n",
        "- Under the hood, a similarity score is computed between the embedded query and all the chunks in the database\n",
        "- The top $k$ chunks with the highest similarity scores are then returned."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7VZaCTQ3cnTn"
      },
      "outputs": [],
      "source": [
        "retriever = vectorstore.as_retriever(\n",
        "    search_type='similarity',\n",
        "    search_kwargs={'k': 3}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxbV5jUTljAr"
      },
      "source": [
        "The given code initializes a **retriever** from the **Chroma vector store** to fetch similar documents based on embeddings. Here's a breakdown:\n",
        "\n",
        "- `vectorstore.as_retriever(...)`: Converts the **Chroma vector store** into a retriever for querying.\n",
        "- `search_type='similarity'`: Specifies that retrieval is based on **cosine similarity** (or another similarity metric used by Chroma).\n",
        "- `search_kwargs={'k': 6}`: Retrieves the **top 6 most similar** documents for a given query.\n",
        "\n",
        "This allows for **efficient information retrieval**, where the retriever finds the most relevant document chunks based on their **semantic similarity** to a user's query."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4iTlNgmcjCA"
      },
      "source": [
        "#### **Retrieving the Relevant Documents**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aAtKGa7cjCA"
      },
      "source": [
        "Let's ask a simple query and see what document chunks are returned based on the similarity search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lBbt7EL-cjCB"
      },
      "outputs": [],
      "source": [
        "user_input = \"How does does Apple develop and ship products that requires good coordination between the teams?\"\n",
        "\n",
        "relevant_document_chunks = retriever.get_relevant_documents(user_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-RVCOBjcjCB"
      },
      "outputs": [],
      "source": [
        "len(relevant_document_chunks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0uOjn_ZfcjCC"
      },
      "outputs": [],
      "source": [
        "for document in relevant_document_chunks:\n",
        "    print(document.page_content.replace(\"\\t\", \" \"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IWmV7lUkcjCC"
      },
      "outputs": [],
      "source": [
        "len(relevant_document_chunks[0].page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yilY2k1UcjCC"
      },
      "source": [
        "It can be observed that the chunks are related to the user query and can perhaps contain the answer."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generation"
      ],
      "metadata": {
        "id": "W-YvSKEayQyw"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s343-PgW6P-K"
      },
      "source": [
        "#### Designing the System Prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcFbX_wRyaWw"
      },
      "source": [
        "System Prompt designing is a crucial part of designing a RAG based system, it consists mainly of two parts:\n",
        "\n",
        "- system message: This is the instruction that has to be given to the LLM.\n",
        "- user message template: This is a message template that contains the context retrieved from the document chunks and the User Query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LR4dzgL96U0-"
      },
      "outputs": [],
      "source": [
        "qna_system_message = \"\"\"\n",
        "You are an assistant whose work is to give answers to questions with repect to a context.\n",
        "User input will have the context required by you to answer user questions.\n",
        "\n",
        "This context will begin with the token: ###Context.\n",
        "The context contains references to specific portions of a document relevant to the user query.\n",
        "\n",
        "User questions will begin with the token: ###Question.\n",
        "\n",
        "Strictly answer only using the information provided in the ###Context.\n",
        "Do not mention anything about the information in ###Context or the question in ###Question in your final answer.\n",
        "\n",
        "If the answer to ###Question cannot be derived from the ###Context, just respond by saying \"I don't know\".\n",
        "\n",
        "Remember that the answer to ###Question might not always be directly present in the information provided in the ###Context.\n",
        "the answer can be indirectly derived from the information in ###Context.\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpTNjk4BJbAO"
      },
      "source": [
        "**Note**: It is important to specify that the LLM should not attempt to answer the question if the context provided (retrieved from the knowledge base provided) doesn't contain the information required. We don't want the LLM to use the knowledge from its training data and/or hallucinate to share a \"seemingly correct\" answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bDexqi8c6Xmm"
      },
      "outputs": [],
      "source": [
        "qna_user_message_template = \"\"\"\n",
        "Conider the following ###Context and ###Question\n",
        "###Context\n",
        "{context}\n",
        "\n",
        "###Question\n",
        "{question}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WriTJFsgyjyG"
      },
      "source": [
        "### Defining the function for generating responses\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brCl4jicyoSV"
      },
      "source": [
        "Let's create a function that takes a user query and an LLM as input, finds the relevant chunks, and uses them as context to generate an answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sbp8L7LixiaC"
      },
      "outputs": [],
      "source": [
        "def generate_rag_response(user_input , llm):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        user_input: Takes a user input for which the response should be retrieved from the vectorDB.\n",
        "        llm: The LLM to be used for generating the response\n",
        "    Returns:\n",
        "        The generated response based on the user query and the context from the knowledge base\n",
        "    \"\"\"\n",
        "    relevant_document_chunks = retriever.get_relevant_documents(user_input)\n",
        "    context_list = [d.page_content.replace(\"\\t\", \" \") for d in relevant_document_chunks]\n",
        "    context_for_query = \". \".join(context_list)\n",
        "\n",
        "\n",
        "\n",
        "    # Combine user_prompt and system_message to create the prompt\n",
        "    prompt = f\"\"\"[INST]{qna_system_message}\\n\n",
        "                {'user'}: {qna_user_message_template.format(context=context_for_query, question=user_input)}\n",
        "                [/INST]\"\"\"\n",
        "\n",
        "\n",
        "    # Quering an LLM\n",
        "    try:\n",
        "        response = llm(\n",
        "                prompt=prompt,\n",
        "                max_tokens=512,\n",
        "                temperature=0.4,\n",
        "                top_p=0.95,\n",
        "                repeat_penalty=1.2,\n",
        "                top_k=25,\n",
        "                stop=['INST'],\n",
        "                echo=False\n",
        "                )\n",
        "\n",
        "        prediction =  response[\"choices\"][0][\"text\"]\n",
        "\n",
        "    except Exception as e:\n",
        "        prediction = f'Sorry, I encountered the following error: \\n {e}'\n",
        "\n",
        "    return  prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tE8uZXT7cyY1"
      },
      "source": [
        "Let's try this function on the previous user query and see whether it can generate an answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffP1SRYbPQHN"
      },
      "source": [
        "# Question Answering using RAG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjajBEj06B0E"
      },
      "source": [
        "### Question 1: Who are the authors of this article and who published this article ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gt4TAQNa6B0E"
      },
      "outputs": [],
      "source": [
        "question_1 = \"Who are the authors of this article and who published this article ?\"\n",
        "rag_answer_1=generate_rag_response(question_1,lcpp_llm)\n",
        "print(rag_answer_1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYAgvE7700Gx"
      },
      "source": [
        "- The answer is clear, concise, and focused, without any unnecessary information.  \n",
        "\n",
        "- For queries like this, we expect a response of this nature."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDw8zXuq6B0F"
      },
      "source": [
        "### Question 2: List down the three leadership characteristics in bulleted points and explain each one of the characteristics under two lines."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i92cv0dQ6B0F"
      },
      "outputs": [],
      "source": [
        "question_2 = \"List down the three leadership characteristics in bulleted points and explain each one of the characteristics under two lines.\"\n",
        "rag_answer_2=generate_rag_response(question_2,lcpp_llm)\n",
        "print(rag_answer_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TggYyQPL6B0G"
      },
      "source": [
        "### Question 3: Can you explain specific examples from the article where Apple's approach to leadership has led to successful innovations?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ed6x6LGb6B0G"
      },
      "outputs": [],
      "source": [
        "question_3 = \"Can you explain specific examples from the article where Apple's approach to leadership has led to successful innovations?\"\n",
        "rag_answer_3=generate_rag_response(question_3,lcpp_llm)\n",
        "print(rag_answer_3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwXBt3hdkMva"
      },
      "source": [
        "# Output Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWgXb9yUe-qN"
      },
      "source": [
        "**Why Do We Need Output Evaluation in a RAG-Based System?**  \n",
        "Output evaluation in a **Retrieval-Augmented Generation (RAG) system** is essential to ensure that the system produces **accurate, relevant, and reliable** responses. Since RAG systems rely on both **retrieval** and **generation**, issues like **irrelevant retrieval, hallucinations, or poor contextual grounding** can degrade the output quality. Evaluating the outputs helps in:  \n",
        "\n",
        "- **Measuring Groundedness** - Ensuring that the generated response is **faithfully derived** from the retrieved documents.  \n",
        "- **Assessing Relevance** - Checking if the retrieved information directly answers the user’s query.    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLM as a Judge"
      ],
      "metadata": {
        "id": "dl5prTYn_oeu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2dDxkZdyKSnh"
      },
      "outputs": [],
      "source": [
        "groundedness_rater_system_message = \"\"\"\n",
        "You will be given a ###Question, ###Context, and an AI-generated ###Answer.\n",
        "\n",
        "Your task: Rate how well the ###Answer is derived from the ###Context.\n",
        "\n",
        "Return only a single number from 1 to 5:\n",
        "1 = Not derived at all\n",
        "2 = Derived to a limited extent\n",
        "3 = Derived to a good extent\n",
        "4 = Derived mostly\n",
        "5 = Derived completely\n",
        "\n",
        "Return only the Score in last in a dictionary format not json and score should be in the range of 1 to 5.\n",
        "Example {groundedness_score:4}\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gy8lOzVg08ca"
      },
      "source": [
        "This prompt is designed to evaluate the groundedness of the AI-generated answer, i.e., how well the answer is derived from the provided context. It asks the LLM judge to compare the answer with the context and rate it on a scale of 1 to 5, where:\n",
        "\n",
        "- 1 indicates no derivation from the context, and\n",
        "- 5 indicates complete derivation from the context.\n",
        "\n",
        "This helps in assessing whether the model is hallucinating or if its response is factually accurate and grounded in the retrieved information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NIosu2Wk7OVs"
      },
      "outputs": [],
      "source": [
        "relevance_rater_system_message = \"\"\"\n",
        "You will be given a ###Question, ###Context, and an AI-generated ###Answer.\n",
        "\n",
        "Your task: Rate how relevant the ###Answer is to the ###Question, based on the ###Context.\n",
        "\n",
        "Return only a single number from 1 to 5:\n",
        "1 = Not relevant at all\n",
        "2 = Slightly relevant, misses key aspects\n",
        "3 = Moderately relevant, addresses some parts but misses important details\n",
        "4 = Mostly relevant, covers key aspects with minor gaps\n",
        "5 = Fully relevant, directly answers all important aspects with details from Context\n",
        "\n",
        "Return only the Score in last in a dictionary format not json and score should be in the range of 1 to 5.\n",
        "Example {relevance_score:4}\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPG_jB1-1Buo"
      },
      "source": [
        "This prompt is focused on evaluating the relevance of the generated answer. It checks if the answer addresses the main aspects of the question using the provided context. The rating is again on a scale of 1 to 5, where:\n",
        "\n",
        "- 1 indicates the answer is irrelevant, and\n",
        "- 5 indicates it is completely relevant to the question.\n",
        "\n",
        "This ensures that the output is not only accurate but also contextually appropriate and directly answers the user's query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7boMupgh_Gux"
      },
      "outputs": [],
      "source": [
        "user_message_template = \"\"\"\n",
        "###Question\n",
        "{question}\n",
        "\n",
        "###Context\n",
        "{context}\n",
        "\n",
        "###Answer\n",
        "{answer}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bNSRk3nyeqC_"
      },
      "outputs": [],
      "source": [
        "def generate_ground_relevance_response(user_input,answer,llm):\n",
        "    global qna_system_message,qna_user_message_template\n",
        "    # Retrieve relevant document chunks\n",
        "    relevant_document_chunks = retriever.get_relevant_documents(query=user_input)\n",
        "    context_list = [d.page_content for d in relevant_document_chunks]\n",
        "    context_for_query = \". \".join(context_list)\n",
        "\n",
        "\n",
        "    # Combine user_prompt and system_message to create the prompt\n",
        "    groundedness_prompt = f\"\"\"[INST]{groundedness_rater_system_message}\\n\n",
        "                {'user'}: {user_message_template.format(context=context_for_query, question=user_input, answer=answer)}\n",
        "                [/INST]\"\"\"\n",
        "\n",
        "    # Combine user_prompt and system_message to create the prompt\n",
        "    relevance_prompt = f\"\"\"[INST]{relevance_rater_system_message}\\n\n",
        "                {'user'}: {user_message_template.format(context=context_for_query, question=user_input, answer=answer)}\n",
        "                [/INST]\"\"\"\n",
        "\n",
        "    response_1 = llm(\n",
        "            prompt=groundedness_prompt,\n",
        "            max_tokens=1024,\n",
        "            temperature= 0.3,\n",
        "            top_p= 0.95,\n",
        "            top_k= 50,\n",
        "            stop=None,\n",
        "            echo=False\n",
        "            )\n",
        "\n",
        "    response_2 = llm(\n",
        "            prompt=relevance_prompt,\n",
        "            max_tokens= 1024,\n",
        "            temperature= 0.3,\n",
        "            top_p= 0.95,\n",
        "            top_k= 50,\n",
        "            stop=None,\n",
        "            echo=False\n",
        "            )\n",
        "\n",
        "    return response_1['choices'][0]['text'],response_2['choices'][0]['text']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Evaluation 1: Base Prompt Response Evaluation**"
      ],
      "metadata": {
        "id": "7hSMSQ5jXggw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ANSXsgwXggx"
      },
      "outputs": [],
      "source": [
        "groundedness_report_1, relevance_report_1 = generate_ground_relevance_response(question_1,base_answer_1,lcpp_llm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80lARDCiXggx"
      },
      "outputs": [],
      "source": [
        "print(groundedness_report_1, '\\n\\n', relevance_report_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oIci9K5bXggy"
      },
      "outputs": [],
      "source": [
        "groundedness_report_2, relevance_report_2 = generate_ground_relevance_response(question_2,base_answer_2,lcpp_llm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6wN1c_E6Xggy"
      },
      "outputs": [],
      "source": [
        "print(groundedness_report_2, '\\n\\n', relevance_report_2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "groundedness_report_3, relevance_report_3 = generate_ground_relevance_response(question_3,base_answer_3,lcpp_llm)"
      ],
      "metadata": {
        "id": "rK1rkvQEXggy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(groundedness_report_3, '\\n\\n', relevance_report_3)"
      ],
      "metadata": {
        "id": "rPUpWNZdXggz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Even after providing a strict prompt instructing the model to return a dictionary, it is still unable to consistently do so in all answers. This may be due to the model's lower instruction-following capability or the limitations of its compressed format (like GGUF), which is a lightweight version of the model designed to run on smaller devices but can sometimes reduce output accuracy."
      ],
      "metadata": {
        "id": "vAaGJZF92aFn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Evaluation 2: RAG Response Evaluation**"
      ],
      "metadata": {
        "id": "h_kKHCe2mqpE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MUHHwLT8iGdp"
      },
      "outputs": [],
      "source": [
        "groundedness_report_1, relevance_report_1 = generate_ground_relevance_response(question_1,rag_answer_1,lcpp_llm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m1CEkU-niXvm"
      },
      "outputs": [],
      "source": [
        "print(groundedness_report_1, '\\n\\n', relevance_report_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f2yqeMJjteWu"
      },
      "outputs": [],
      "source": [
        "groundedness_report_2, relevance_report_2 = generate_ground_relevance_response(question_2,rag_answer_2,lcpp_llm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vE8oZ8SxtiYP"
      },
      "outputs": [],
      "source": [
        "print(groundedness_report_2, '\\n\\n', relevance_report_2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "groundedness_report_3, relevance_report_3 = generate_ground_relevance_response(question_3,rag_answer_3,lcpp_llm)"
      ],
      "metadata": {
        "id": "snzm4LnqlMbn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(groundedness_report_3, '\\n\\n', relevance_report_3)"
      ],
      "metadata": {
        "id": "Mse2QJF2lNdt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The results from the LLM Judge show that responses generated using RAG consistently outperform those without RAG across both evaluation metrics: groundedness and relevance."
      ],
      "metadata": {
        "id": "l5FOvhTq8A4U"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMCN2_H204M_"
      },
      "source": [
        "# **Conclusion**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDuEjGBuPsUm"
      },
      "source": [
        "- We've learned how to create a Retrieval-Augmented Generation (RAG) based application that can perform Q&A from documents for accurate information retrieval.\n",
        "    - First, we chunked the data to create multiple splits with overlaps.\n",
        "    - Then we used embedding models to encode the different data splits.\n",
        "    - Then we stored these embeddings in a vector database.\n",
        "    - Then we defined an LLM that would take the user query and relevant context via the encoded data chunks.\n",
        "    - Finally, we assembled all these components to build the RAG-based system.\n",
        "- We've also learned how to evaluate the output of a RAG-based system using the LLM-as-a-Judge technique to check the groundedness and relevance of the generated output.\n",
        "- Lastly, we also compared the output from an LLM and that from an RAG-based system and understood the differences in the groundedness and relevance of the two methods."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQvaNDqQ3BJa"
      },
      "source": [
        "<font size = 6 color ='#4682B4' > Power Ahead </font>\n",
        "___"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "49Hj1OD_kkWG",
        "CYKE35BRkspB",
        "3f5p5bLolH7f",
        "UgAFFFruPmpH",
        "eVUcJXbLYYp0",
        "DQ7lR9WLlOTh",
        "y5_a6Hv4k84d",
        "t63YyaeidcQ-",
        "JKoXO1OYCXs2",
        "CdB5Kr5B-EsZ",
        "Egn9xt_B9Muk",
        "f57VodWykVxA",
        "W-YvSKEayQyw",
        "ffP1SRYbPQHN",
        "OwXBt3hdkMva",
        "RMCN2_H204M_"
      ],
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}