{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sW4FpPGRQhmx"
      },
      "source": [
        "<center><img src=\"https://upload.wikimedia.org/wikipedia/commons/e/e9/4_RGB_McCombs_School_Brand_Branded.png\" width=\"300\" height=\"100\"/>\n",
        "  <img src=\"https://mma.prnewswire.com/media/1458111/Great_Learning_Logo.jpg?p=facebook\" width=\"200\" height=\"100\"/></center>\n",
        "\n",
        "<center><font size=10>Generative AI for Business Applications</center></font>\n",
        "<center><font size=6>Retrieval Augmented Generation - Week 3</center></font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1EgG25UQ3PX"
      },
      "source": [
        "<center><img src=\"https://i.ibb.co/pBF9nKpf/apple.png\" width=\"720\"></center>\n",
        "\n",
        "<center><font size=6>Apple HBR Report Document Q&A</center></font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgLfhM7hStpK"
      },
      "source": [
        "# Problem Statement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49Hj1OD_kkWG"
      },
      "source": [
        "## Business Context"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pegcATWWStpK"
      },
      "source": [
        "As organizations grow and scale, they are often inundated with large volumes of data, reports, and documents that contain critical information for decision-making. In real-world business settings, such as venture capital firms like Andreesen Horowitz, business analysts are required to sift through large datasets, research papers, or reports to extract relevant information that impacts strategic decisions.\n",
        "\n",
        "For instance, consider that you've just joined Andreesen Horowitz, a renowned venture capital firm, and you are tasked with analyzing a dense report like the Harvard Business Review's **\"How Apple is Organized for Innovation.\"** Going through the report manually can be extremely time-consuming as the size and complexity of these report increases. However, by using **Semantic Search** and **Retrieval-Augmented Generation (RAG)** models, you can significantly streamline this process.\n",
        "\n",
        "Imagine having the capability to directly ask questions like, “How does Apple structure its teams for innovation?” and get immediate, relevant answers drawn from the report. This ability to extract and organize specific insights quickly and accurately enables you to focus on higher-level analysis and decision-making, rather than being bogged down by information retrieval."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYKE35BRkspB"
      },
      "source": [
        "## Objective"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcksbDgdlHhA"
      },
      "source": [
        "The goal is to develop a RAG application that helps business analysts efficiently extract key insights from extensive reports, such as “How Apple is Organized for Innovation.”\n",
        "\n",
        "Specifically, the system aims to:\n",
        "\n",
        "- Answer user queries by retrieving relevant content directly from lengthy documents.\n",
        "\n",
        "- Support natural-language interaction without requiring a full manual read-through.\n",
        "\n",
        "- Act as an intelligent assistant that streamlines the report analysis process.\n",
        "\n",
        "Through this solution, analysts can save time, improve productivity, and make faster, more informed strategic decisions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3f5p5bLolH7f"
      },
      "source": [
        "## Data Description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KagUxRh3lKiv"
      },
      "source": [
        "**How Apple is Organized for Innovation** - An article of 11 pages in pdf format"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgAFFFruPmpH"
      },
      "source": [
        "# Installing and Importing the Necessary Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yC_5gAl0PoPM"
      },
      "outputs": [],
      "source": [
        "# Install required libraries\n",
        "!pip install -q langchain_community==0.3.27 \\\n",
        "              langchain==0.3.27 \\\n",
        "              chromadb==1.0.15 \\\n",
        "              pymupdf==1.26.3 \\\n",
        "              tiktoken==0.9.0 \\\n",
        "              ragas==0.3.0 \\\n",
        "              datasets==4.0.0 \\\n",
        "              evaluate==0.4.5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CG4B9XCIpEy3"
      },
      "source": [
        "**Note**:\n",
        "- After running the above cell, kindly restart the runtime (for Google Colab) or notebook kernel (for Jupyter Notebook), and run all cells sequentially from the next cell.\n",
        "- On executing the above line of code, you might see a warning regarding package dependencies. This error message can be ignored as the above code ensures that all necessary libraries and their dependencies are maintained to successfully execute the code in ***this notebook***."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HKMjqQpFl4kd"
      },
      "outputs": [],
      "source": [
        "# Import core libraries\n",
        "import os                                                                       # Interact with the operating system (e.g., set environment variables)\n",
        "import json                                                                     # Read/write JSON data\n",
        "\n",
        "# Import libraries for working with PDFs and OpenAI\n",
        "from langchain.document_loaders import PyMuPDFLoader                            # Load and extract text from PDF files\n",
        "from openai import OpenAI                                                       # Access OpenAI's models and services\n",
        "\n",
        "# Import libraries for processing dataframes and text\n",
        "import tiktoken                                                                 # Tokenizer used for counting and splitting text for models\n",
        "import pandas as pd                                                             # Load, manipulate, and analyze tabular data\n",
        "\n",
        "# Import LangChain components for data loading, chunking, embedding, and vector DBs\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter              # Break text into overlapping chunks for processing\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings                        # Create vector embeddings using OpenAI's models  # type: ignore\n",
        "from langchain.vectorstores import Chroma                                       # Store and search vector embeddings using Chroma DB  # type: ignore\n",
        "\n",
        "# Import components to run evaluation on RAG pipeline outputs\n",
        "from ragas import evaluate\n",
        "from ragas.metrics import (\n",
        "    Faithfulness,\n",
        "    AnswerRelevancy,\n",
        "    LLMContextPrecisionWithoutReference,\n",
        ")\n",
        "from datasets import Dataset                                                    # Used to structure the input (questions, answers, contexts etc.) in tabular format\n",
        "from langchain_openai import ChatOpenAI                                         # This is needed since LLM is used in metric computation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KVNQv8HasUni"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVUcJXbLYYp0"
      },
      "source": [
        "# Loading the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b3wGwCZjo3Zh"
      },
      "outputs": [],
      "source": [
        "# uncomment and run the below code snippets if the dataset is present in the Google Drive\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FLkk3OF1YXVJ"
      },
      "outputs": [],
      "source": [
        "pdf_file = \"/content/HBR_How_Apple_Is_Organized_For_Innovation.pdf\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KltKUFH5YXVK"
      },
      "outputs": [],
      "source": [
        "pdf_loader = PyMuPDFLoader(pdf_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uq1lhM4WFTS2"
      },
      "source": [
        "### OpenAI API Calling\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the JSON file and extract values\n",
        "file_name = 'config.json'                                                       # Name of the configuration file\n",
        "with open(file_name, 'r') as file:                                              # Open the config file in read mode\n",
        "    config = json.load(file)                                                    # Load the JSON content as a dictionary\n",
        "    OPENAI_API_KEY = config.get(\"OPENAI_API_KEY\")                                             # Extract the API key from the config\n",
        "    OPENAI_API_BASE = config.get(\"OPENAI_API_BASE\")                             # Extract the OpenAI base URL from the config\n",
        "\n",
        "# Store API credentials in environment variables\n",
        "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY                                          # Set API key as environment variable\n",
        "os.environ[\"OPENAI_BASE_URL\"] = OPENAI_API_BASE                                 # Set API base URL as environment variable\n",
        "\n",
        "# Initialize OpenAI client\n",
        "client = OpenAI()                                                               # Create an instance of the OpenAI client"
      ],
      "metadata": {
        "id": "eMi5GjvNBrlO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qfdmzUEl5lQ"
      },
      "source": [
        "# Question Answering using Base Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response(user_input,k=5,max_tokens=500,temperature=0.3,top_p=0.95):\n",
        "    prompt=\"Answer the question\"\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": prompt},\n",
        "            {\"role\": \"user\", \"content\": user_input}\n",
        "        ],\n",
        "        max_tokens=max_tokens,\n",
        "        temperature=temperature,\n",
        "        top_p=top_p\n",
        "        )\n",
        "        # Extract and print the generated text from the response\n",
        "        response = response.choices[0].message.content.strip()\n",
        "    except Exception as e:\n",
        "        response = f'Sorry, I encountered the following error: \\n {e}'\n",
        "\n",
        "    return response"
      ],
      "metadata": {
        "id": "Wg2VSU34mAWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSfYbmwTl5lQ"
      },
      "source": [
        "### Question 1: Who are the authors of this article and who published this article ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zm0ibudOl5lR"
      },
      "outputs": [],
      "source": [
        "question_1 = \"Who are the authors of this article and who published this article ?\"\n",
        "base_answer_1=generate_response(question_1)\n",
        "print(base_answer_1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rM4TsE6Il5lR"
      },
      "source": [
        "### Question 2: List down the three leadership characteristics in bulleted points and explain each one of the characteristics under two lines."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DYgFuzp_l5lR"
      },
      "outputs": [],
      "source": [
        "question_2 = \"List down the three leadership characteristics in bulleted points and explain each one of the characteristics under two lines.\"\n",
        "base_answer_2=generate_response(question_2)\n",
        "print(base_answer_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHsMbu7Tl5lS"
      },
      "source": [
        "### Question 3: Can you explain specific examples from the article where Apple's approach to leadership has led to successful innovations?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nfHMDHlGl5lS"
      },
      "outputs": [],
      "source": [
        "question_3 = \"Can you explain specific examples from the article where Apple's approach to leadership has led to successful innovations?\"\n",
        "base_answer_3=generate_response(question_3)\n",
        "print(base_answer_3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t63YyaeidcQ-"
      },
      "source": [
        "# Retrieval Augmented Generation Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKoXO1OYCXs2"
      },
      "source": [
        "### Split the Loaded PDF into Chunks for Further Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EJXwUPWCxM8J"
      },
      "outputs": [],
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    encoding_name='cl100k_base',\n",
        "    chunk_size=512,\n",
        "    chunk_overlap=20\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8QosusIl6F1"
      },
      "source": [
        "The given code initializes a **RecursiveCharacterTextSplitter** to split the text into manageable chunks for embedding and retrieval. Here's a breakdown:\n",
        "\n",
        "- `RecursiveCharacterTextSplitter.from_tiktoken_encoder(...)`: Uses **TikToken encoding** to properly handle token-based splitting.\n",
        "- `encoding_name='cl100k_base'`: Specifies the **TikToken encoding** (used by OpenAI models like GPT-4 and GPT-3.5).\n",
        "- `chunk_size=512`: Each text chunk will have a maximum of **512 tokens**.\n",
        "- `chunk_overlap=16`: Ensures **overlapping** of 16 tokens between consecutive chunks to preserve context.\n",
        "\n",
        "This approach ensures that text is split **intelligently** while maintaining **semantic meaning** for better retrieval and embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fp9yToSobbZu"
      },
      "outputs": [],
      "source": [
        "document_chunks = pdf_loader.load_and_split(text_splitter)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9ny5vsSVUS8"
      },
      "source": [
        "(Note: Expect that the above cell will take time to execute)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WewdBkHrD6pA"
      },
      "source": [
        "Let's take a look at consecutive chunks from the document."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vjAO9Z84EHnA"
      },
      "outputs": [],
      "source": [
        "i = 5\n",
        "document_chunks[i]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nLuUFIza5aAk"
      },
      "outputs": [],
      "source": [
        "document_chunks[i+1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rrmpizT6qrP"
      },
      "source": [
        "As we can see there is some overlap between the chunks.\n",
        "- This improves the coherence and relevance of retrieved results, as the model can better understand the relationship between adjacent parts of the document.\n",
        "- It also helps in maintaining the flow of ideas and ensuring that critical context is available when generating answers, leading to more accurate and contextually consistent outputs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdB5Kr5B-EsZ"
      },
      "source": [
        "### Generate Vector Embeddings for Text Chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nwusGdTRxhhP"
      },
      "outputs": [],
      "source": [
        "# Initialize the OpenAI Embeddings model with API credentials\n",
        "embedding_model = OpenAIEmbeddings(\n",
        "    openai_api_key=OPENAI_API_KEY,                                                     # Your OpenAI API key for authentication\n",
        "    openai_api_base=OPENAI_API_BASE                                             # The OpenAI API base URL endpoint\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxVDXdV2imGE"
      },
      "source": [
        "Now that we have chunked the raw input, **we can add these chunks to an embedding model and then store the generated embeddings into a vector database.**\n",
        "  - We generate a vector for each chunk and save this chunk along with the vector representation in a specialized database."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Egn9xt_B9Muk"
      },
      "source": [
        "### Creating a Vector Database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NFbwm8l1sW1J"
      },
      "outputs": [],
      "source": [
        "out_dir = 'apple_db'\n",
        "\n",
        "if not os.path.exists(out_dir):\n",
        "  os.makedirs(out_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "972yZSXwcdpH"
      },
      "outputs": [],
      "source": [
        "vectorstore = Chroma.from_documents(\n",
        "    document_chunks,\n",
        "    embedding_model,\n",
        "    persist_directory=out_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNbhfJ3VlZzJ"
      },
      "source": [
        "The given code initializes a **vector database** (also called **vector store**) using **Chroma**, a popular open-source vector database, to store **document embeddings** for retrieval in a **Retrieval-Augmented Generation (RAG)** system. Here's a breakdown of what each part does:  \n",
        "\n",
        "- **`vectorstore = Chroma.from_documents(...)`**  \n",
        "   - This creates a **Chroma vector store** from a set of **document chunks**. Chroma is used to store and retrieve embeddings efficiently.\n",
        "\n",
        "- **Parameters Passed to `Chroma.from_documents()`**  \n",
        "   - `document_chunks`: A list of **text chunks** (split portions of a document) that will be converted into embeddings.  \n",
        "   - `embedding_model`: The model responsible for **embedding** the document chunks into vector representations. Common choices include OpenAI’s embeddings, Sentence Transformers, or other dense vector models.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DEECCFPqsex6"
      },
      "outputs": [],
      "source": [
        "vectorstore = Chroma(persist_directory=out_dir,embedding_function=embedding_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f57VodWykVxA"
      },
      "source": [
        "### Retrieval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aROMSqTJFJd2"
      },
      "source": [
        "We will now create a retriever that can query an input text and retrieve the top-k documents that are most relevant from the vector store.\n",
        "\n",
        "- Under the hood, a similarity score is computed between the embedded query and all the chunks in the database\n",
        "- The top k chunks with the highest similarity scores are then returned."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7VZaCTQ3cnTn"
      },
      "outputs": [],
      "source": [
        "retriever = vectorstore.as_retriever(\n",
        "    search_type='similarity',\n",
        "    search_kwargs={'k': 4}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxbV5jUTljAr"
      },
      "source": [
        "The given code initializes a **retriever** from the **Chroma vector store** to fetch similar documents based on embeddings. Here's a breakdown:\n",
        "\n",
        "- `vectorstore.as_retriever(...)`: Converts the **Chroma vector store** into a retriever for querying.\n",
        "- `search_type='similarity'`: Specifies that retrieval is based on **cosine similarity** (or another similarity metric used by Chroma).\n",
        "- `search_kwargs={'k': 6}`: Retrieves the **top 6 most similar** documents for a given query.\n",
        "\n",
        "This allows for **efficient information retrieval**, where the retriever finds the most relevant document chunks based on their **semantic similarity** to a user's query."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4iTlNgmcjCA"
      },
      "source": [
        "#### **Retrieving the Relevant Documents**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aAtKGa7cjCA"
      },
      "source": [
        "Let's ask a simple query and see what document chunks are returned based on the similarity search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lBbt7EL-cjCB"
      },
      "outputs": [],
      "source": [
        "user_input = \"Who are the authors of this article and who published this article ?\"\n",
        "\n",
        "relevant_document_chunks = retriever.get_relevant_documents(user_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-RVCOBjcjCB"
      },
      "outputs": [],
      "source": [
        "len(relevant_document_chunks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0uOjn_ZfcjCC"
      },
      "outputs": [],
      "source": [
        "for document in relevant_document_chunks:\n",
        "    print(document.page_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IWmV7lUkcjCC"
      },
      "outputs": [],
      "source": [
        "len(relevant_document_chunks[0].page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yilY2k1UcjCC"
      },
      "source": [
        "It can be observed that the chunks are related to the user query and can perhaps contain the answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pv9oH4ukXLSa"
      },
      "source": [
        "### Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s343-PgW6P-K"
      },
      "source": [
        "#### Designing the System Prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8ZO9sWaZ9KH"
      },
      "source": [
        "System Prompt designing is a crucial part of designing a RAG based system, it consists mainly of two parts:\n",
        "\n",
        "- system message: This is the instruction that has to be given to the LLM.\n",
        "- user message template: This is a message template that contains the context retrieved from the document chunks and the User Query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LR4dzgL96U0-"
      },
      "outputs": [],
      "source": [
        "qna_system_message = \"\"\"\n",
        "You are an assistant whose work is to give answers to questions with repect to a context.\n",
        "User input will have the context required by you to answer user questions.\n",
        "\n",
        "This context will begin with the token: ###Context.\n",
        "The context contains references to specific portions of a document relevant to the user query.\n",
        "\n",
        "User questions will begin with the token: ###Question.\n",
        "\n",
        "Strictly answer only using the information provided in the ###Context.\n",
        "Do not mention anything about the information in ###Context or the question in ###Question in your final answer.\n",
        "\n",
        "If the answer to ###Question cannot be derived from the ###Context, just respond by saying \"I don't know\".\n",
        "\n",
        "Remember that the answer to ###Question might not always be directly present in the information provided in the ###Context.\n",
        "the answer can be indirectly derived from the information in ###Context.\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpTNjk4BJbAO"
      },
      "source": [
        "**Note**: It is important to specify that the LLM should not attempt to answer the question if the context provided (retrieved from the knowledge base provided) doesn't contain the information required. We don't want the LLM to use the knowledge from its training data and/or hallucinate to share a \"seemingly correct\" answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bDexqi8c6Xmm"
      },
      "outputs": [],
      "source": [
        "qna_user_message_template = \"\"\"\n",
        "Conider the following ###Context and ###Question\n",
        "###Context\n",
        "{context}\n",
        "\n",
        "###Question\n",
        "{question}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_ekBjVM60P1"
      },
      "source": [
        "### Defining the function for generating responses\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDxq9l75ccSU"
      },
      "source": [
        "Let's create a function that takes a user query and an LLM as input, finds the relevant chunks, and uses them as context to generate an answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sbp8L7LixiaC"
      },
      "outputs": [],
      "source": [
        "def generate_rag_response(user_input,k=5,max_tokens=500,temperature=0.3,top_p=0.95):\n",
        "    global qna_system_message,qna_user_message_template\n",
        "    # Retrieve relevant document chunks\n",
        "    relevant_document_chunks = retriever.get_relevant_documents(query=user_input,k=k)\n",
        "    context_list = [d.page_content for d in relevant_document_chunks]\n",
        "\n",
        "    # Combine document chunks into a single context\n",
        "    context_for_query = \". \".join(context_list)\n",
        "\n",
        "    user_message = qna_user_message_template.replace('{context}', context_for_query)\n",
        "    user_message = user_message.replace('{question}', user_input)\n",
        "\n",
        "    # Generate the response\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": qna_system_message},\n",
        "            {\"role\": \"user\", \"content\": user_message}\n",
        "        ],\n",
        "        max_tokens=max_tokens,\n",
        "        temperature=temperature,\n",
        "        top_p=top_p\n",
        "        )\n",
        "        # Extract and print the generated text from the response\n",
        "        response = response.choices[0].message.content.strip()\n",
        "    except Exception as e:\n",
        "        response = f'Sorry, I encountered the following error: \\n {e}'\n",
        "\n",
        "    return response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tE8uZXT7cyY1"
      },
      "source": [
        "Let's try this function on the previous user query and see whether it can generate an answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffP1SRYbPQHN"
      },
      "source": [
        "# Question Answering using RAG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjajBEj06B0E"
      },
      "source": [
        "### Question 1: Who are the authors of this article and who published this article ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gt4TAQNa6B0E"
      },
      "outputs": [],
      "source": [
        "question_1 = \"Who are the authors of this article and who published this article ?\"\n",
        "rag_answer_1=generate_rag_response(question_1)\n",
        "print(rag_answer_1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDw8zXuq6B0F"
      },
      "source": [
        "### Question 2: List down the three leadership characteristics in bulleted points and explain each one of the characteristics under two lines."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i92cv0dQ6B0F"
      },
      "outputs": [],
      "source": [
        "question_2 = \"List down the three leadership characteristics in bulleted points and explain each one of the characteristics under two lines.\"\n",
        "rag_answer_2=generate_rag_response(question_2)\n",
        "print(rag_answer_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TggYyQPL6B0G"
      },
      "source": [
        "### Question 3: Can you explain specific examples from the article where Apple's approach to leadership has led to successful innovations?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ed6x6LGb6B0G"
      },
      "outputs": [],
      "source": [
        "question_3 = \"Can you explain specific examples from the article where Apple's approach to leadership has led to successful innovations?\"\n",
        "rag_answer_3=generate_rag_response(question_3)\n",
        "print(rag_answer_3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwXBt3hdkMva"
      },
      "source": [
        "# Output Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWgXb9yUe-qN"
      },
      "source": [
        "**Why Do We Need These Evaluation Metrics in a RAG-Based System?**\n",
        "When evaluating a RAG system, using multiple metrics helps us capture different aspects of response quality. Each metric plays a distinct role in identifying weaknesses and ensuring the system produces trustworthy outputs.\n",
        "\n",
        "* **Faithfulness** - Checks whether the generated response stays true to the retrieved context without adding unsupported or hallucinated information.\n",
        "* **Answer Relevancy** - Measures how directly the response addresses the user's query, ensuring that the answer is not only correct but also useful.\n",
        "* **Context Precision** - Evaluates how precisely the retrieved context contributes to answering the query, reducing noise and irrelevant details."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating Responses using RAGAS"
      ],
      "metadata": {
        "id": "K6O9uuUK_tLN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation 1: Base Prompt Response Evaluation"
      ],
      "metadata": {
        "id": "h_kKHCe2mqpE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the evaluator LLM\n",
        "evaluator_llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0)\n",
        "\n",
        "# Initialize evaluation metrics\n",
        "faithfulness = Faithfulness()\n",
        "answer_relevancy = AnswerRelevancy()\n",
        "context_precision = LLMContextPrecisionWithoutReference()"
      ],
      "metadata": {
        "id": "lLYiNtUO_vG5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "questions = [question_1,question_2,question_3]                            # List of user questions\n",
        "responses_with_base = [base_answer_1,base_answer_2,base_answer_3]               # Responses from Base Model\n",
        "\n",
        "# Retrieve top-k documents as context for each question\n",
        "contexts = [\n",
        "    [doc.page_content for doc in retriever.get_relevant_documents(q, k=6)]      # Get top 6 docs for each question\n",
        "    for q in questions\n",
        "]"
      ],
      "metadata": {
        "id": "5EYG4SP0vqA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Wrap into HuggingFace Dataset\n",
        "ragas_dataset_with_RAG = Dataset.from_dict({\n",
        "    \"question\": questions,\n",
        "    \"answer\": responses_with_base,\n",
        "    \"contexts\": contexts,\n",
        "    \"reference\": questions\n",
        "})\n",
        "\n",
        "# Run RAGAS evaluation\n",
        "result_with_rag = evaluate(\n",
        "    ragas_dataset_with_RAG,\n",
        "    metrics=[\n",
        "        answer_relevancy,\n",
        "        context_precision,\n",
        "        faithfulness,\n",
        "    ],\n",
        "    llm=evaluator_llm,\n",
        "    embeddings=embedding_model\n",
        ")\n",
        "\n",
        "# Convert results to DataFrame\n",
        "df_rag = result_with_rag.to_pandas()\n",
        "df_rag"
      ],
      "metadata": {
        "id": "PO5EEq2cvqA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation 2: RAG Response Evaluation"
      ],
      "metadata": {
        "id": "mhL7eflwW1_4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "questions = [question_1,question_2,question_3]                                    # List of user questions\n",
        "responses_with_rag = [rag_answer_1,rag_answer_2,rag_answer_3]                     # Responses from RAG pipeline\n",
        "\n",
        "# Retrieve top-k documents as context for each question\n",
        "contexts = [\n",
        "    [doc.page_content for doc in retriever.get_relevant_documents(q, k=6)]        # Get top 6 docs for each question\n",
        "    for q in questions\n",
        "]"
      ],
      "metadata": {
        "id": "rB1a7VlUCnjM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Wrap into HuggingFace Dataset\n",
        "ragas_dataset_with_RAG = Dataset.from_dict({\n",
        "    \"question\": questions,\n",
        "    \"answer\": responses_with_rag,\n",
        "    \"contexts\": contexts,\n",
        "    \"reference\": questions\n",
        "})\n",
        "\n",
        "# Run RAGAS evaluation\n",
        "result_with_rag = evaluate(\n",
        "    ragas_dataset_with_RAG,\n",
        "    metrics=[\n",
        "        answer_relevancy,\n",
        "        context_precision,\n",
        "        faithfulness,\n",
        "    ],\n",
        "    llm=evaluator_llm,\n",
        "    embeddings=embedding_model\n",
        ")\n",
        "\n",
        "# Convert results to DataFrame\n",
        "df_rag = result_with_rag.to_pandas()\n",
        "df_rag"
      ],
      "metadata": {
        "id": "iNs3CMlWC_1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The results from the RAGAS model show that responses generated using RAG consistently outperform those from the base model across all three evaluation metrics: answer relevancy, LLM context precision (without reference), and faithfulness"
      ],
      "metadata": {
        "id": "f9D8EKxd3Ftb"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMCN2_H204M_"
      },
      "source": [
        "# **Conclusion**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDuEjGBuPsUm"
      },
      "source": [
        "* We've learned how to create a Retrieval-Augmented Generation (RAG) based application using an **OpenAI model** that can perform Q\\&A from documents for accurate information retrieval.\n",
        "\n",
        "  * First, we chunked the data to create multiple splits with overlaps.\n",
        "  * Then we used embedding models to encode the different data splits.\n",
        "  * Then we stored these embeddings in a vector database.\n",
        "  * Then we defined the OpenAI model that would take the user query and relevant context via the encoded data chunks.\n",
        "  * Finally, we assembled all these components to build the RAG-based system.\n",
        "* We've also learned how to evaluate the output of a RAG-based system using the **RAGAS framework**, which measures groundedness, relevance, and answer correctness.\n",
        "* Lastly, we also compared the output from an OpenAI model alone and that from an RAG-based system and understood the differences in the faithfullness, answer relevancy  and context precision of the two methods.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQvaNDqQ3BJa"
      },
      "source": [
        "<font size = 6 color = '#4682B4' > Power Ahead </font>\n",
        "___"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "UgAFFFruPmpH",
        "eVUcJXbLYYp0",
        "Uq1lhM4WFTS2",
        "JKoXO1OYCXs2",
        "CdB5Kr5B-EsZ",
        "Egn9xt_B9Muk",
        "pv9oH4ukXLSa",
        "ffP1SRYbPQHN"
      ],
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}