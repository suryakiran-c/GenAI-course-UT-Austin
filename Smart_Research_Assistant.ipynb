{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "GO3jBIhW3pSj",
        "nx8AaUozAdZd",
        "l54U7FCxDnDl",
        "17VcIdIcDadL",
        "5DSuuldPKKId"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmKLmjqmHMQM"
      },
      "source": [
        "<center><p float=\"center\">\n",
        "  <img src=\"https://upload.wikimedia.org/wikipedia/commons/e/e9/4_RGB_McCombs_School_Brand_Branded.png\" width=\"300\" height=\"100\"/>\n",
        "  <img src=\"https://mma.prnewswire.com/media/1458111/Great_Learning_Logo.jpg?p=facebook\" width=\"200\" height=\"100\"/>\n",
        "</p></center>\n",
        "\n",
        "<center><font size=10>Generative AI for Business Applications</center></font>\n",
        "<center><font size=6>Transformers for Text Generation - Week 1</center></font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azxWym9-HMpz"
      },
      "source": [
        "<center><p float=\"center\">\n",
        "  <img src=\"https://images.pexels.com/photos/4050287/pexels-photo-4050287.jpeg\" width=480></a>\n",
        "<center><font size=6>Smart Research Assistant</center></font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aa1yXUesL7Zy"
      },
      "source": [
        "# Problem Statement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFN8LrodMR7w"
      },
      "source": [
        "## Business Context"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In todayâ€™s fast-paced tech industry, employees must stay informed about the latest technologies, trends, and innovations. However, the overwhelming volume of online content news, blogs, and reports makes it difficult to quickly extract relevant insights.\n",
        "\n",
        "To address this, the broader vision is to build an **Intelligence Platform** that enables tech professionals to access concise summaries of technical articles and get precise answers to their questions streamlining, information flow and saving valuable time.\n",
        "\n"
      ],
      "metadata": {
        "id": "wwelbXBnOTbZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Objective"
      ],
      "metadata": {
        "id": "qDG_248npouB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal is to develop a prototype that demonstrates how Natural Language Processing (NLP) can support tech employees in efficiently extracting insights from lengthy technical articles.\n",
        "\n",
        "Specifically, the system aims to:\n",
        "\n",
        "* Summarize technical content into concise, relevant overviews to reduce reading time.\n",
        "* Answer user queries based on article content, simulating an intelligent information assistant.\n",
        "\n",
        "This project focuses on building a small part of that broader solution, a simple system where a user submits an article, and the model either summarizes it or answers specific questions based on its content. This enables employees to quickly understand complex articles, boosting productivity and saving valuable research time.\n",
        "\n",
        "Through the successful implementation of this platform, the organization seeks to enhance its overall operational efficiency, drive innovation, and maintain a competitive edge in the rapidly changing tech landscape.\n"
      ],
      "metadata": {
        "id": "uMU1_PHEXMHt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Description"
      ],
      "metadata": {
        "id": "IGxmR0XVwcAi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset has two columns:\n",
        "\n",
        "* **Title**: A brief headline summarizing the main topic of the article.\n",
        "* **Article**: Full unstructured text containing the detailed content."
      ],
      "metadata": {
        "id": "AMTN_p_Hfi-Q"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YNy7qFuKmeO"
      },
      "source": [
        "# Importing Necessary Libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We install specific tested library versions to ensure compatibility and avoid errors during development.\n"
      ],
      "metadata": {
        "id": "3zNGV_EpXU0P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iBROi0RwwGuD"
      },
      "outputs": [],
      "source": [
        "!pip install \\\n",
        "    numpy==1.26.4 \\\n",
        "    transformers==4.53.1 \\\n",
        "    pandas==2.2.2 \\\n",
        "    streamlit==1.47.0 \\\n",
        "    torch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note:\n",
        "- After running the above cell, kindly restart the runtime (for Google Colab) or notebook kernel (for Jupyter Notebook), and run all cells sequentially from the next cell.\n",
        "- On executing the above line of code, you might see a warning regarding package dependencies. This error message can be ignored as the above code ensures that all necessary libraries and their dependencies are maintained to successfully execute the code in this notebook."
      ],
      "metadata": {
        "id": "4EEnxPxFSCw-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Prompt**:\n",
        "\n",
        "<font size=3 color=\"#4682B4\"><b>I want to analyze the provided CSV data and build a Text Summarizer using language model from Hugging Face Transformers. Help me import the necessary Python libraries to:\n",
        "\n",
        "1. Read and manipulate the data\n",
        "2. Load the language model using AutoModelForCausalLM ,AutoModelForSeq2SeqLM and AutoTokenizer\n",
        "3. Use torch for model inference\n",
        "4. Suppress unnecessary warnings for a cleaner output\n",
        "\n",
        "</font>"
      ],
      "metadata": {
        "id": "RM5RW5-0TopC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer , AutoModelForSeq2SeqLM\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "lvyxAIh7dz_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtegCA1CK5wE"
      },
      "source": [
        "# Loading the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Weâ€™ll use the Pandas library to load the data. Pandas makes it easy to work with tables of data and take a quick look at whatâ€™s inside.\n",
        "\n",
        "Letâ€™s load the dataset and see what it looks like."
      ],
      "metadata": {
        "id": "RyWaBCwIPz4F"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "er-hahR_HS7L"
      },
      "source": [
        "***Prompt***:\n",
        "\n",
        "<font size=3 color=\"#4682B4\"><b> Mount the Google Drive\n",
        "</font>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "yvmBPiheIm4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JhIr_HKrCN5"
      },
      "source": [
        "***Prompt***:\n",
        "\n",
        "<font size=3 color=\"#4682B4\"><b> Load the CSV file named \"Articles\" and store it in the variable data.\n",
        "</font>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/Articles.csv')\n"
      ],
      "metadata": {
        "id": "5HZqLoC4rPFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3v-5s3Z4LA1o"
      },
      "source": [
        "# Data Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23rEgLArjbuo"
      },
      "source": [
        "***Prompt***:\n",
        "\n",
        "<font size=3 color=\"#4682B4\"><b> Display the number of rows and columns in the `data`.\n",
        "</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AYv3A1qRZHzB"
      },
      "outputs": [],
      "source": [
        "data.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset consists of 10 rows and 2 columns"
      ],
      "metadata": {
        "id": "CpyA0k2rHb4f"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0J9f9vmyHir0"
      },
      "source": [
        "***Prompt***:\n",
        "\n",
        "<font size=3 color=\"#4682B4\"><b> Display the first 5 rows of the `data`.\n",
        "</font>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.head(5)\n"
      ],
      "metadata": {
        "id": "3YfdoVt0xsPi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9y4AluIJJ3D"
      },
      "source": [
        "***Prompt***:\n",
        "\n",
        "<font size=3 color=\"#4682B4\"><b> Display the names, data types, and number of entries in the columns of the `data`.\n",
        "</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFjcDNy-peDG"
      },
      "outputs": [],
      "source": [
        "data.info()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Loading"
      ],
      "metadata": {
        "id": "dR6U80hvsK6F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use the **Hugging Face Transformers** library, a popular open-source platform that provides access to thousands of pre-trained LLM models. It simplifies the process of working with complex transformer architectures by offering:\n",
        "\n",
        "* A unified API for models like FLAN T5, Tiny Llama, Mistral, etc.\n",
        "* Easy model loading from the Hugging Face Hub"
      ],
      "metadata": {
        "id": "M4geIVClsiC3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can explore thousands of open-source models and datasets on the [Hugging Face Model Hub](https://huggingface.co/)\n"
      ],
      "metadata": {
        "id": "XVlVaH-axw-x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case study, we experiment with the following pre-trained models:\n",
        "\n",
        "1. `google/flan-t5-large`\n",
        "2. `TinyLlama/TinyLlama-1.1B-Chat-v1.0`"
      ],
      "metadata": {
        "id": "zq3mvjKUyYBa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up Hugging Face token"
      ],
      "metadata": {
        "id": "gkuJOprEkmjX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Follow these steps to securely set up your Hugging Face token\n",
        "\n",
        "**Step 1: Create Your Token on Hugging Face**\n",
        "\n",
        "1. Visit: [https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)\n",
        "2. Click on **\"New token\"**\n",
        "3. Give it a name (e.g., `colab_token`)\n",
        "4. Set the role as **\"Read\"**\n",
        "5. Click **\"Create token\"**\n",
        "6. **Copy** the token that gets generated"
      ],
      "metadata": {
        "id": "COt7wG9vj9Zv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2: Store Your Token Securely in config file**\n",
        "\n",
        "Paste your token securely in the config file provided."
      ],
      "metadata": {
        "id": "1Gt_Ijugyv83"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2: Run the code below to store your HF_TOKEN securely in os enviroment:**"
      ],
      "metadata": {
        "id": "tYtFdazkzIJ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "# Load from your config.json (update the path if needed)\n",
        "with open(\"config.json\", \"r\") as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "# Extract the token\n",
        "hf_token = config.get(\"HF_TOKEN\")\n",
        "\n",
        "# Store in Colab's environment variables\n",
        "os.environ[\"HF_TOKEN\"] = hf_token\n"
      ],
      "metadata": {
        "id": "JnYhm0-dzDns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Youâ€™re now ready to use Hugging Face models!"
      ],
      "metadata": {
        "id": "A5gn-9IukE_6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FLAN-T5 Large"
      ],
      "metadata": {
        "id": "dvOWspAyyidf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are different ways to load AI models, and each model may have its own specific method depending on how it is built and shared. So it's best to follow the official Hugging Face documentation to load it correctly."
      ],
      "metadata": {
        "id": "D883i859hvJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can search for the model on Hugging Face or use this link:\n",
        "ðŸ”— [https://huggingface.co/google/flan-t5-large](https://huggingface.co/google/flan-t5-large)\n",
        "\n",
        "Once you're on the model page:\n",
        "\n",
        "* Go to the **\"Use this model\"** section, and from the dropdown, choose **\"Use in Colab\"** or **\"Use in Kaggle\"** to see example code for loading the model.\n",
        "\n",
        "* This will open ready-made code examples that show you how to load and run the model\n",
        "* You can follow the instructions directly to avoid errors\n",
        "\n",
        "Checking the documentation is always a good practice to make sure you are using the model in the recommended way.\n"
      ],
      "metadata": {
        "id": "AY_IfQQvhstK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In our case, weâ€™re using the **FLAN-T5 large** and the **TinyLlama** from **Hugging Face**."
      ],
      "metadata": {
        "id": "5G7jvsfFh0w7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**FLAN-T5 (google/flan-t5-large)**\n",
        "\n",
        "1. An **encoder-decoder** model designed for sequence-to-sequence tasks like summarization and question answering.\n",
        "2. Trained using **instruction tuning**, allowing it to follow specific input prompts and generate structured, task-specific outputs.\n",
        "3. FLAN-T5 Large supports a context window of up to 512 tokens (only input)\n"
      ],
      "metadata": {
        "id": "dIln8JrFKGH6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDjUfRIOzuWn"
      },
      "source": [
        "***Prompt***:\n",
        "\n",
        "<font size=3 color=\"#4682B4\"><b> Load the `google/flan-t5-large` from hugging face\n",
        "</font>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"google/flan-t5-large\"\n",
        "tokenizer_flant5 = AutoTokenizer.from_pretrained(model_name)\n",
        "model_flant5 = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "XGME2IqSwamk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TinyLlama"
      ],
      "metadata": {
        "id": "eHUMhGOZy79R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TinyLlama (TinyLlama-1.1B-Chat-v1.0)**\n",
        "\n",
        "1. It is a **decoder-only model**, trained using causal language modeling.\n",
        "2. It generates fluent and conversational text, optimized for speed and efficiency on smaller hardware.\n",
        "3. TinyLlama is a **causal language model (CLM)**, meaning it is trained to predict the next word given previous words only.\n",
        "4. TinyLlama supports a context window of up to 2,048 tokens, which includes both input and output combined.\n",
        "\n"
      ],
      "metadata": {
        "id": "SXLG0tlJKLJG"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLj-CZgQz9_W"
      },
      "source": [
        "***Prompt***:\n",
        "\n",
        "<font size=3 color=\"#4682B4\"><b> Load the `TinyLlama/TinyLlama-1.1B-Chat-v1.0` from hugging face\n",
        "</font>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name_tinyllama = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "tokenizer_tinyllama = AutoTokenizer.from_pretrained(model_name_tinyllama)\n",
        "model_tinyllama = AutoModelForCausalLM.from_pretrained(model_name_tinyllama)"
      ],
      "metadata": {
        "id": "cQDVwJ_q0Gyh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Summarization"
      ],
      "metadata": {
        "id": "l_EMKz6DIE8C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To begin the summarization process, we extract two articles from our dataset and store it in a variable called sample_article. This article will be used as the input for testing the model's output.\n",
        "\n"
      ],
      "metadata": {
        "id": "mPi8gfe5clpr"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9KTCEeOdBWv"
      },
      "source": [
        "***Prompt***:\n",
        "\n",
        "<font size=3 color=\"#4682B4\"><b> Select first article from the dataset and store it in a variable named sample_article_1.\n",
        "</font>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_article_1 = data['Article'][0]\n",
        "sample_article_1"
      ],
      "metadata": {
        "id": "_kxdcrBPcWOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbz9wF_Y05sD"
      },
      "source": [
        "***Prompt***:\n",
        "\n",
        "<font size=3 color=\"#4682B4\"><b> Select sixth article  from the dataset and store it in a variable named sample_article_2.\n",
        "</font>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_article_2 = data['Article'][5]\n",
        "sample_article_2"
      ],
      "metadata": {
        "id": "TD3Y5hM605sD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FLAN-T5 Large"
      ],
      "metadata": {
        "id": "QF3EawvDKi7F"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HC-5A0Yec_xC"
      },
      "source": [
        "***Prompt***:\n",
        "\n",
        "<font size=3 color=\"#4682B4\"><b>Create a function that takes an article as input and returns its summary using the FLAN-T5 model.\n",
        "\n",
        "</font>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_flant5(article):\n",
        "    input_text = f\"Summarize the following article, ensuring all key details and insights are captured clearly and concisely in the summary : {article}\"\n",
        "    inputs = tokenizer_flant5(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "    outputs = model_flant5.generate(inputs[\"input_ids\"], max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "    summary = tokenizer_flant5.decode(outputs[0], skip_special_tokens=True)\n",
        "    return summary"
      ],
      "metadata": {
        "id": "r866BCloIGoW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Article 1"
      ],
      "metadata": {
        "id": "uNG5B3fw1KuW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output=summarize_flant5(sample_article_1)\n",
        "print(output)"
      ],
      "metadata": {
        "id": "_x6DxdLSIXJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Response**\n",
        "\n",
        "> **Article** : In todayâ€™s hyper-connected digital age, the tech industry is evolving at a rapid pace, with a constant influx of information from news articles, research papers, press releases, social media, and analyst reports. While this stream of data holds immense strategic value, it also presents a serious challenge: information overload. Tech companies are struggling to manage the volume, relevance, and timeliness of this information, leading to delays in product development, missed market signals, and ineffective strategic planning. The problem is compounded by the fragmented nature of sources and the difficulty in distinguishing valuable insights from repetitive or shallow content. Traditionally, companies have relied on analysts to manually scan, read, and summarize this information, a process that is not only slow and resource-intensive but also prone to human bias and oversight. This is where Natural Language Processing (NLP) emerges as a transformative solution. With state-of-the-art transformer models like BART, T5, and GPT, companies can now automatically summarize long-form content, extract key entities and keywords, detect sentiment, and cluster related topics for pattern recognition. These capabilities enable faster, more accurate synthesis of competitive and market intelligence. By integrating such NLP tools into a centralized platform, companies can automate data ingestion, filtering, summarization, and alerting, ensuring that only the most relevant insights reach decision-makers in real time. This shift from manual to AI-assisted intelligence allows organizations to be more agile, make faster decisions, and gain a strategic edge. Real-world examples already show companies leveraging NLP to track competitor activity, summarize industry trends, and deliver tailored updates to different departments, significantly improving internal communication and responsiveness. Whether building an in-house platform or adopting third-party tools, the key lies in customizing the NLP system to align with strategic goals, data privacy needs, and the pace of market evolution. As information volumes continue to rise, the real advantage will belong to organizations that stop trying to read everything and start using AI to read smartly. In doing so, they will not only manage information overload but turn it into a powerful asset for innovation and long-term competitiveness.\n",
        "\n",
        "> **Summary** : Using Natural Language Processing (NLP) to read smartly will help organizations manage information overload and turn it into a powerful asset for innovation and long-term competitiveness. Using NLP to read smartly will help organizations manage information overload and turn it into a powerful asset for innovation and long-term competitiveness."
      ],
      "metadata": {
        "id": "vzBfwfuQ1Thr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Article 2"
      ],
      "metadata": {
        "id": "DEsPlcFa1Odn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output=summarize_flant5(sample_article_2)\n",
        "print(output)"
      ],
      "metadata": {
        "id": "Z2PnL3ij1ROJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Response**\n",
        "\n",
        "> **Article** : For many decision-makers across tech companiesâ€”whether in executive leadership, product strategy, or innovation managementâ€”Natural Language Processing (NLP) has emerged as one of the most transformative yet misunderstood technologies in modern business intelligence. At its core, NLP is the ability of machines to read, understand, and generate human language, enabling companies to extract meaning from massive volumes of unstructured text data. Unlike traditional analytics that work best on numbers and structured databases, NLP specializes in processing content that humans create every dayâ€”news stories, product reviews, market reports, social media posts, and internal documentation. This makes it uniquely suited to the kinds of insights that matter most to business leaders: what customers are saying, what competitors are announcing, how sentiment is shifting, and where market conversations are headed. Modern NLP is powered by large language models (LLMs) like GPT-4, BERT, RoBERTa, and T5, which use deep learning and transformer architectures to understand context, intent, and nuance. These models can automatically summarize 20-page reports into five-sentence briefs, extract key information such as competitor names, product launches, and pricing changes, and even classify documents by theme, urgency, or relevance. For decision-makers, this means less time reading and more time actingâ€”receiving focused updates rather than digging through dozens of scattered documents or emails. For example, a CEO might get a daily digest highlighting any strategic moves made by top five competitors, while a CMO could track which messaging trends are gaining traction in recent tech product launches. NLP tools can also scan earnings calls or investor briefings to detect hidden concerns, confidence levels, or repeated talking points that signal a shift in company direction. What makes NLP especially powerful is its scalability and adaptabilityâ€”once set up, it can monitor thousands of sources simultaneously, in multiple languages, across global markets. And because these tools are trainable, they can learn what matters most to a specific organization, tailoring summaries and filters accordingly. Importantly, NLP doesnâ€™t replace human judgmentâ€”it enhances it by delivering sharper, faster, and more relevant inputs. For decision-makers looking to navigate uncertainty, outmaneuver competitors, and capitalize on early signals, NLP offers a strategic capability that goes far beyond automationâ€”it provides clarity, context, and competitive advantage in an increasingly noisy world.\n",
        "\n",
        "> **Summary** : Outmaneu is a technology that enables companies to extract meaningful insights from unstructured text data. Outmaneu is a technology that enables companies to extract meaningful insights from unstructured text data. Outmaneu is a technology that enables companies to extract meaningful insights from unstructured text data.\n"
      ],
      "metadata": {
        "id": "sHkiSWBX2dli"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Result"
      ],
      "metadata": {
        "id": "3PNySsHk3imE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Both the summaries appear repetitive because FLAN-T5 has a maximum input limit of 512 tokens, whereas the articles being provided exceed that limit. As a result, much of the input gets truncated, leading to incomplete context and repetitive outputs."
      ],
      "metadata": {
        "id": "0ki7zKD65ERw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Limitation of FLAN-T5**\n",
        "\n",
        "1. **Older Architecture**\n",
        "   FLAN-T5 is an older encoder-decoder model and may underperform compared to newer models fine-tuned for nuanced generation.\n",
        "\n",
        "2. **Fewer Parameters**\n",
        "   FLAN-T5-large has fewer parameters and less training data, limiting its ability to handle complex or detailed content.\n",
        "\n",
        "3. **Context Limit of 512 Tokens**\n",
        "   The model can only process up to 512 tokens at once. Inputs beyond this (e.g., 1000+ word articles) may be truncated, leading to incomplete summaries.\n"
      ],
      "metadata": {
        "id": "FFSSPZz-1o7e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To handle the context window limitation, we can use larger variants like **FLAN-T5 XL** or **FLAN-T5 XXL**, which support significantly higher token limits (up to 2048 tokens), allowing for more complete input processing and improved summary quality.\n"
      ],
      "metadata": {
        "id": "vfbVUwEtZGgd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Having explored the limitations of FLAN-T5, letâ€™s now test the TinyLlama model, which supports a higher token limit of 2048, allowing better handling of longer article inputs.\n",
        "\n",
        "Letâ€™s see how it performs on the same article using a summarization or question-answering approach.\n"
      ],
      "metadata": {
        "id": "nM5blHsMIg3F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tiny Llama"
      ],
      "metadata": {
        "id": "0p4-8pXqdkv_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlJfVcySdnGf"
      },
      "source": [
        "***Prompt***:\n",
        "\n",
        "<font size=3 color=\"#4682B4\"><b> Create a function that takes an article as input and returns its summary using the Tiny Llama model.\n",
        "</font>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_tinyllama(article):\n",
        "    # For causal models like TinyLlama, summarization isn't a direct task like with encoder-decoder models.\n",
        "    # We can prompt it to continue a summary.\n",
        "    prompt=\"Summarize the following article clearly and concisely:\"\n",
        "    input_text = f\"{prompt}\\n{article}\\nSummary:\"\n",
        "    inputs = tokenizer_tinyllama(input_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
        "\n",
        "    # Generate tokens - the model will try to complete the input prompt.\n",
        "    # We need to adjust generation parameters for open-ended generation.\n",
        "    # max_new_tokens controls how much new text is generated after the prompt.\n",
        "    outputs = model_tinyllama.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        attention_mask=inputs[\"attention_mask\"],\n",
        "        max_new_tokens=500,  # Generate up to 300 new tokens for the summary\n",
        "        do_sample=True,     # Don't sample, use greedy decoding\n",
        "        temperature=0.7,\n",
        "        min_new_tokens=150,\n",
        "        top_p=0.9,\n",
        "        pad_token_id=tokenizer_tinyllama.eos_token_id, # Pad with EOS token if needed\n",
        "    )\n",
        "\n",
        "    # Decode the entire output sequence.\n",
        "    generated_text = tokenizer_tinyllama.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # The generated text will include the original prompt. We need to extract the summary part.\n",
        "    # This is a simple approach, more sophisticated parsing might be needed depending on prompt and output.\n",
        "    summary_start_index = generated_text.find(\"Summary:\") + len(\"Summary:\")\n",
        "    summary = generated_text[summary_start_index:].strip()\n",
        "\n",
        "    return summary"
      ],
      "metadata": {
        "id": "8W5pd4i1KyA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Article 1"
      ],
      "metadata": {
        "id": "GO3jBIhW3pSj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output=summarize_tinyllama(sample_article_1)\n",
        "print(output)"
      ],
      "metadata": {
        "id": "19oXcwcD3pSj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Response**\n",
        "\n",
        "> **Article** : In todayâ€™s hyper-connected digital age, the tech industry is evolving at a rapid pace, with a constant influx of information from news articles, research papers, press releases, social media, and analyst reports. While this stream of data holds immense strategic value, it also presents a serious challenge: information overload. Tech companies are struggling to manage the volume, relevance, and timeliness of this information, leading to delays in product development, missed market signals, and ineffective strategic planning. The problem is compounded by the fragmented nature of sources and the difficulty in distinguishing valuable insights from repetitive or shallow content. Traditionally, companies have relied on analysts to manually scan, read, and summarize this information, a process that is not only slow and resource-intensive but also prone to human bias and oversight. This is where Natural Language Processing (NLP) emerges as a transformative solution. With state-of-the-art transformer models like BART, T5, and GPT, companies can now automatically summarize long-form content, extract key entities and keywords, detect sentiment, and cluster related topics for pattern recognition. These capabilities enable faster, more accurate synthesis of competitive and market intelligence. By integrating such NLP tools into a centralized platform, companies can automate data ingestion, filtering, summarization, and alerting, ensuring that only the most relevant insights reach decision-makers in real time. This shift from manual to AI-assisted intelligence allows organizations to be more agile, make faster decisions, and gain a strategic edge. Real-world examples already show companies leveraging NLP to track competitor activity, summarize industry trends, and deliver tailored updates to different departments, significantly improving internal communication and responsiveness. Whether building an in-house platform or adopting third-party tools, the key lies in customizing the NLP system to align with strategic goals, data privacy needs, and the pace of market evolution. As information volumes continue to rise, the real advantage will belong to organizations that stop trying to read everything and start using AI to read smartly. In doing so, they will not only manage information overload but turn it into a powerful asset for innovation and long-term competitiveness.\n",
        "\n",
        "> **Summary** : The article highlights the challenges that the tech industry faces in managing information overload. It explains how NLP technology can help companies automate data ingestion, filtering, summarization, and alerting. The article also provides real-world examples of how AI is used to track competitor activity, summarize industry trends, and deliver tailored updates to different departments. The article emphasizes the importance of customizing the NLP system to align with strategic goals, data privacy needs, and the pace of market evolution. The article suggests that organizations that stop trying to read everything and start using AI to read smartly will not only manage information overload but also turn it into a powerful asset for innovation and long-term competitiveness."
      ],
      "metadata": {
        "id": "mRZuwi923pSk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Article 2"
      ],
      "metadata": {
        "id": "Qtfb8ahP5Qws"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output=summarize_tinyllama(sample_article_2)\n",
        "print(output)"
      ],
      "metadata": {
        "id": "x9ft2dbE5Qwt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Response**\n",
        "\n",
        "> **Article** : For many decision-makers across tech companiesâ€”whether in executive leadership, product strategy, or innovation managementâ€”Natural Language Processing (NLP) has emerged as one of the most transformative yet misunderstood technologies in modern business intelligence. At its core, NLP is the ability of machines to read, understand, and generate human language, enabling companies to extract meaning from massive volumes of unstructured text data. Unlike traditional analytics that work best on numbers and structured databases, NLP specializes in processing content that humans create every dayâ€”news stories, product reviews, market reports, social media posts, and internal documentation. This makes it uniquely suited to the kinds of insights that matter most to business leaders: what customers are saying, what competitors are announcing, how sentiment is shifting, and where market conversations are headed. Modern NLP is powered by large language models (LLMs) like GPT-4, BERT, RoBERTa, and T5, which use deep learning and transformer architectures to understand context, intent, and nuance. These models can automatically summarize 20-page reports into five-sentence briefs, extract key information such as competitor names, product launches, and pricing changes, and even classify documents by theme, urgency, or relevance. For decision-makers, this means less time reading and more time actingâ€”receiving focused updates rather than digging through dozens of scattered documents or emails. For example, a CEO might get a daily digest highlighting any strategic moves made by top five competitors, while a CMO could track which messaging trends are gaining traction in recent tech product launches. NLP tools can also scan earnings calls or investor briefings to detect hidden concerns, confidence levels, or repeated talking points that signal a shift in company direction. What makes NLP especially powerful is its scalability and adaptabilityâ€”once set up, it can monitor thousands of sources simultaneously, in multiple languages, across global markets. And because these tools are trainable, they can learn what matters most to a specific organization, tailoring summaries and filters accordingly. Importantly, NLP doesnâ€™t replace human judgmentâ€”it enhances it by delivering sharper, faster, and more relevant inputs. For decision-makers looking to navigate uncertainty, outmaneuver competitors, and capitalize on early signals, NLP offers a strategic capability that goes far beyond automationâ€”it provides clarity, context, and competitive advantage in an increasingly noisy world.\n",
        "\n",
        "> **Summary** : The article discusses the transformative power of Natural Language Processing (NLP) in modern business intelligence. It highlights how NLP has emerged as a critical tool for decision-makers across tech companies, enabling them to extract meaning from massive volumes of unstructured text data. The article explains how NLP is powered by large language models (LLMs) like GPT-4, BERT, RoBERTa, and T5, which use deep learning and transformer architectures to understand context, intent, and nuance. The article also highlights the importance of scalability and adaptability in NLP, as these tools can monitor thousands of sources simultaneously, in multiple languages, and adapt to different organizations' needs. The article concludes by emphasizing the strategic capability of NLP, which provides clarity, context, and competitive advantage in an increasingly noisy world."
      ],
      "metadata": {
        "id": "EDYWXnN45Qwt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Result"
      ],
      "metadata": {
        "id": "vZnYfUAo_H-d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- We observe that TinyLlama is capable of generating high-quality summaries that effectively capture the key points and important details from the original content.\n",
        "- The summaries are concise yet informative, demonstrating the modelâ€™s ability to retain context and highlight the most relevant information.\n",
        "- This makes TinyLlama a useful tool for tasks that require quick understanding of lengthy texts."
      ],
      "metadata": {
        "id": "5Gsdk8Bc_H-e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question-Answering"
      ],
      "metadata": {
        "id": "26dD08-45RwC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FLAN-T5 Large"
      ],
      "metadata": {
        "id": "nx8AaUozAdZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We observed that FLAN-T5 struggles to handle longer text inputs effectively, which is why we are not using it to generate answers in this case."
      ],
      "metadata": {
        "id": "jWyYMgsaAyAl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tiny Llama"
      ],
      "metadata": {
        "id": "PxwnYLGvAx36"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maksY55GXGiY"
      },
      "source": [
        "***Prompt***:\n",
        "\n",
        "<font size=3 color=\"#4682B4\"><b> Create a function that takes an article and a question as input, and returns the answer generated by the TinyLlama model.\n",
        "</font>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def answer_question_tinyllama(article, question):\n",
        "    # Formulate the prompt to guide the TinyLlama model to answer the question based on the article.\n",
        "    # We ask the model to act as an AI answering a question based on the provided text.\n",
        "    input_text = f\"From this Article: {article}\\n\\n Answer the below Question: {question}\\n\\nAnswer:\"\n",
        "\n",
        "    # Tokenize the input text\n",
        "    # Truncate if the combined article and question is too long\n",
        "    inputs = tokenizer_tinyllama(input_text, return_tensors=\"pt\", max_length=1024, truncation=True ,padding=True)\n",
        "\n",
        "    # Generate the answer using the model.\n",
        "    # We use generate with parameters suitable for generating a concise answer.\n",
        "    outputs = model_tinyllama.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        attention_mask=inputs[\"attention_mask\"],\n",
        "        max_new_tokens=500,  # Generate up to 100 new tokens for the answer\n",
        "        do_sample=True,      # Use sampling to potentially get more varied answers\n",
        "        temperature=0.7,     # Control randomness\n",
        "        top_p=0.9,           # Nucleus sampling\n",
        "        pad_token_id=tokenizer_tinyllama.eos_token_id, # Pad with EOS token if needed\n",
        "    )\n",
        "\n",
        "    # Decode the generated sequence\n",
        "    generated_text = tokenizer_tinyllama.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # The generated text will include the original prompt. We need to extract the answer part.\n",
        "    # This is a simple approach, more sophisticated parsing might be needed depending on prompt and output.\n",
        "    answer_start_index = generated_text.find(\"Answer:\") + len(\"Answer:\")\n",
        "    answer = generated_text[answer_start_index:].strip()\n",
        "\n",
        "    # Basic cleanup: remove potential repetition of the question or prompt in the answer\n",
        "    if answer.startswith(question):\n",
        "        answer = answer[len(question):].strip()\n",
        "\n",
        "    return answer\n"
      ],
      "metadata": {
        "id": "ZXczSOY4OG2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Article 1"
      ],
      "metadata": {
        "id": "rDN7Hf7kBIDs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Question : What are the models that are trending currently?"
      ],
      "metadata": {
        "id": "uxwmP2myn2TP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example Usage:\n",
        "sample_question = \"What are the models that are trending currently?\"\n",
        "answer = answer_question_tinyllama(sample_article_1, sample_question)\n",
        "print(f\"Answer: {answer}\")"
      ],
      "metadata": {
        "id": "uOhuqTuYz-6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Article** : In todayâ€™s hyper-connected digital age, the tech industry is evolving at a rapid pace, with a constant influx of information from news articles, research papers, press releases, social media, and analyst reports. While this stream of data holds immense strategic value, it also presents a serious challenge: information overload. Tech companies are struggling to manage the volume, relevance, and timeliness of this information, leading to delays in product development, missed market signals, and ineffective strategic planning. The problem is compounded by the fragmented nature of sources and the difficulty in distinguishing valuable insights from repetitive or shallow content. Traditionally, companies have relied on analysts to manually scan, read, and summarize this information, a process that is not only slow and resource-intensive but also prone to human bias and oversight. This is where Natural Language Processing (NLP) emerges as a transformative solution. With state-of-the-art transformer models like BART, T5, and GPT, companies can now automatically summarize long-form content, extract key entities and keywords, detect sentiment, and cluster related topics for pattern recognition. These capabilities enable faster, more accurate synthesis of competitive and market intelligence. By integrating such NLP tools into a centralized platform, companies can automate data ingestion, filtering, summarization, and alerting, ensuring that only the most relevant insights reach decision-makers in real time. This shift from manual to AI-assisted intelligence allows organizations to be more agile, make faster decisions, and gain a strategic edge. Real-world examples already show companies leveraging NLP to track competitor activity, summarize industry trends, and deliver tailored updates to different departments, significantly improving internal communication and responsiveness. Whether building an in-house platform or adopting third-party tools, the key lies in customizing the NLP system to align with strategic goals, data privacy needs, and the pace of market evolution. As information volumes continue to rise, the real advantage will belong to organizations that stop trying to read everything and start using AI to read smartly. In doing so, they will not only manage information overload but turn it into a powerful asset for innovation and long-term competitiveness.\n",
        "\n",
        "> Question : What are the models that are trending currently?\n",
        "\n",
        "> Answer : The models that are trending currently are BART, T5, and GPT."
      ],
      "metadata": {
        "id": "WqBQWjOqBUF8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Question : Why there is a need of Text Summarization?"
      ],
      "metadata": {
        "id": "AK79rpcbn8z1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example Usage:\n",
        "sample_question = \"Why there is a need of Text Summarization?\"\n",
        "answer = answer_question_tinyllama(sample_article_1, sample_question)\n",
        "print(f\"Answer: {answer}\")"
      ],
      "metadata": {
        "id": "fXsokvuJoEte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Article** : In todayâ€™s hyper-connected digital age, the tech industry is evolving at a rapid pace, with a constant influx of information from news articles, research papers, press releases, social media, and analyst reports. While this stream of data holds immense strategic value, it also presents a serious challenge: information overload. Tech companies are struggling to manage the volume, relevance, and timeliness of this information, leading to delays in product development, missed market signals, and ineffective strategic planning. The problem is compounded by the fragmented nature of sources and the difficulty in distinguishing valuable insights from repetitive or shallow content. Traditionally, companies have relied on analysts to manually scan, read, and summarize this information, a process that is not only slow and resource-intensive but also prone to human bias and oversight. This is where Natural Language Processing (NLP) emerges as a transformative solution. With state-of-the-art transformer models like BART, T5, and GPT, companies can now automatically summarize long-form content, extract key entities and keywords, detect sentiment, and cluster related topics for pattern recognition. These capabilities enable faster, more accurate synthesis of competitive and market intelligence. By integrating such NLP tools into a centralized platform, companies can automate data ingestion, filtering, summarization, and alerting, ensuring that only the most relevant insights reach decision-makers in real time. This shift from manual to AI-assisted intelligence allows organizations to be more agile, make faster decisions, and gain a strategic edge. Real-world examples already show companies leveraging NLP to track competitor activity, summarize industry trends, and deliver tailored updates to different departments, significantly improving internal communication and responsiveness. Whether building an in-house platform or adopting third-party tools, the key lies in customizing the NLP system to align with strategic goals, data privacy needs, and the pace of market evolution. As information volumes continue to rise, the real advantage will belong to organizations that stop trying to read everything and start using AI to read smartly. In doing so, they will not only manage information overload but turn it into a powerful asset for innovation and long-term competitiveness.\n",
        "\n",
        "> Question : Why there is a need of Text Summarization?\n",
        "\n",
        "> Answer : Text summarization is a process of extracting key ideas, concepts, and information from long-form textual content. It is a crucial step in the process of information retrieval, analysis, and decision-making. The need for text summarization arises due to the large volume of unstructured data generated by various sources such as news articles, research papers, press releases, social media, and analyst reports. This data is often difficult to analyze and extract relevant insights from. Traditionally, organizations have relied on manual summarization methods such as scanning, reading, and summarizing content. However, this process is time-consuming, resource-intensive, and prone to human bias and oversight. With the advent of natural language processing (NLP) technologies, organizations can now automate text summarization tasks. These technologies enable faster, more accurate, and more efficient summarization of competitive and market intelligence. By integrating NLP into a centralized platform, organizations can automate data ingestion, filtering, summarization, and alerting, ensuring that only the most relevant insights reach decision-makers in real-time. This shift from manual to AI-assisted intelligence allows organizations to be more agile, make faster decisions, and gain a strategic edge."
      ],
      "metadata": {
        "id": "O6HVJTSuBoXf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Article 2"
      ],
      "metadata": {
        "id": "nSFtuNQ-B93J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Question : How is NLP different from traditional analytics in terms of data types it handles?"
      ],
      "metadata": {
        "id": "0gA16QrVB93K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example Usage:\n",
        "sample_question = \"How is NLP different from traditional analytics in terms of data types it handles?\"\n",
        "answer = answer_question_tinyllama(sample_article_2, sample_question)\n",
        "print(f\"Answer: {answer}\")"
      ],
      "metadata": {
        "id": "_mxlhCX8B93K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Article** : For many decision-makers across tech companiesâ€”whether in executive leadership, product strategy, or innovation managementâ€”Natural Language Processing (NLP) has emerged as one of the most transformative yet misunderstood technologies in modern business intelligence. At its core, NLP is the ability of machines to read, understand, and generate human language, enabling companies to extract meaning from massive volumes of unstructured text data. Unlike traditional analytics that work best on numbers and structured databases, NLP specializes in processing content that humans create every dayâ€”news stories, product reviews, market reports, social media posts, and internal documentation. This makes it uniquely suited to the kinds of insights that matter most to business leaders: what customers are saying, what competitors are announcing, how sentiment is shifting, and where market conversations are headed. Modern NLP is powered by large language models (LLMs) like GPT-4, BERT, RoBERTa, and T5, which use deep learning and transformer architectures to understand context, intent, and nuance. These models can automatically summarize 20-page reports into five-sentence briefs, extract key information such as competitor names, product launches, and pricing changes, and even classify documents by theme, urgency, or relevance. For decision-makers, this means less time reading and more time actingâ€”receiving focused updates rather than digging through dozens of scattered documents or emails. For example, a CEO might get a daily digest highlighting any strategic moves made by top five competitors, while a CMO could track which messaging trends are gaining traction in recent tech product launches. NLP tools can also scan earnings calls or investor briefings to detect hidden concerns, confidence levels, or repeated talking points that signal a shift in company direction. What makes NLP especially powerful is its scalability and adaptabilityâ€”once set up, it can monitor thousands of sources simultaneously, in multiple languages, across global markets. And because these tools are trainable, they can learn what matters most to a specific organization, tailoring summaries and filters accordingly. Importantly, NLP doesnâ€™t replace human judgmentâ€”it enhances it by delivering sharper, faster, and more relevant inputs. For decision-makers looking to navigate uncertainty, outmaneuver competitors, and capitalize on early signals, NLP offers a strategic capability that goes far beyond automationâ€”it provides clarity, context, and competitive advantage in an increasingly noisy world.\n",
        "\n",
        "> Question : How is NLP different from traditional analytics in terms of data types it handles?\n",
        "\n",
        "> Answer : Natural Language Processing (NLP) is a type of artificial intelligence (AI) that specializes in processing text data. Unlike traditional analytics, which focuses on numbers and structured databases, NLP uses natural language to analyze human-generated content. NLP tools can read, understand, and generate human language, enabling companies to extract meaning from massive volumes of unstructured text data. Unlike traditional analytics that work best on numbers and structured databases, NLP specializes in processing content that humans create every dayâ€”news stories, product reviews, market reports, social media posts, and internal documentation. This makes it uniquely suited to the kinds of insights that matter most to business leaders: what customers are saying, what competitors are announcing, how sentiment is shifting, and where market conversations are headed."
      ],
      "metadata": {
        "id": "ankL_JxZB93M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Question : Why is NLP seen as a strategic tool rather than just automation?"
      ],
      "metadata": {
        "id": "w-LhLICoB93M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example Usage:\n",
        "sample_question = \"Why is NLP seen as a strategic tool rather than just automation?\"\n",
        "answer = answer_question_tinyllama(sample_article_2, sample_question)\n",
        "print(f\"Answer: {answer}\")"
      ],
      "metadata": {
        "id": "8E6iSJrxB93M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Article** : For many decision-makers across tech companiesâ€”whether in executive leadership, product strategy, or innovation managementâ€”Natural Language Processing (NLP) has emerged as one of the most transformative yet misunderstood technologies in modern business intelligence. At its core, NLP is the ability of machines to read, understand, and generate human language, enabling companies to extract meaning from massive volumes of unstructured text data. Unlike traditional analytics that work best on numbers and structured databases, NLP specializes in processing content that humans create every dayâ€”news stories, product reviews, market reports, social media posts, and internal documentation. This makes it uniquely suited to the kinds of insights that matter most to business leaders: what customers are saying, what competitors are announcing, how sentiment is shifting, and where market conversations are headed. Modern NLP is powered by large language models (LLMs) like GPT-4, BERT, RoBERTa, and T5, which use deep learning and transformer architectures to understand context, intent, and nuance. These models can automatically summarize 20-page reports into five-sentence briefs, extract key information such as competitor names, product launches, and pricing changes, and even classify documents by theme, urgency, or relevance. For decision-makers, this means less time reading and more time actingâ€”receiving focused updates rather than digging through dozens of scattered documents or emails. For example, a CEO might get a daily digest highlighting any strategic moves made by top five competitors, while a CMO could track which messaging trends are gaining traction in recent tech product launches. NLP tools can also scan earnings calls or investor briefings to detect hidden concerns, confidence levels, or repeated talking points that signal a shift in company direction. What makes NLP especially powerful is its scalability and adaptabilityâ€”once set up, it can monitor thousands of sources simultaneously, in multiple languages, across global markets. And because these tools are trainable, they can learn what matters most to a specific organization, tailoring summaries and filters accordingly. Importantly, NLP doesnâ€™t replace human judgmentâ€”it enhances it by delivering sharper, faster, and more relevant inputs. For decision-makers looking to navigate uncertainty, outmaneuver competitors, and capitalize on early signals, NLP offers a strategic capability that goes far beyond automationâ€”it provides clarity, context, and competitive advantage in an increasingly noisy world.\n",
        "\n",
        "> Question : Why is NLP seen as a strategic tool rather than just automation?\n",
        "\n",
        "> Answer : NLP is seen as a strategic tool rather than just automation because it offers a way to gather, analyze, and act on vast amounts of unstructured text data. NLP specializes in processing content that humans create every day, enabling companies to extract meaning from it. This makes it uniquely suited to the kinds of insights that matter most to business leaders: what customers are saying, what competitors are announcing, how sentiment is shifting, and where market conversations are headed. NLP tools can scan earnings calls or investor briefings to detect hidden concerns, confidence levels, or repeated talking points that signal a shift in company direction.\n"
      ],
      "metadata": {
        "id": "IvureuqiB93M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### We can see that TinyLlama is generating answers to the questions very effectively."
      ],
      "metadata": {
        "id": "HSB9vcrFJLV3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deployment"
      ],
      "metadata": {
        "id": "Zmr3wnbT_jAX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we deploy this using Hugging Face Spaces with Streamlit.\n",
        "This is purely for demonstration purposes, so we wonâ€™t dive deep into the code here."
      ],
      "metadata": {
        "id": "SXKQbrnoMmxX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Streamlit on Hugging Face"
      ],
      "metadata": {
        "id": "yHKUaar2KtDi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### app.py"
      ],
      "metadata": {
        "id": "l54U7FCxDnDl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile app.py\n",
        "\n",
        "# import streamlit as st\n",
        "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "# import torch\n",
        "# import os\n",
        "# from huggingface_hub import login\n",
        "\n",
        "\n",
        "# model_name_tinyllama = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "# tokenizer_tinyllama = AutoTokenizer.from_pretrained(model_name_tinyllama)\n",
        "# model_tinyllama = AutoModelForCausalLM.from_pretrained(model_name_tinyllama,torch_dtype=torch.float32,device_map={\"\": \"cpu\"})\n",
        "\n",
        "# def summarize_tinyllama(article):\n",
        "#     # For causal models like TinyLlama, summarization isn't a direct task like with encoder-decoder models.\n",
        "#     # We can prompt it to continue a summary.\n",
        "#     prompt=\"Summarize the following article clearly and concisely:\"\n",
        "#     input_text = f\"{prompt}\\n{article}\\nSummary:\"\n",
        "#     inputs = tokenizer_tinyllama(input_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
        "\n",
        "#     # Generate tokens - the model will try to complete the input prompt.\n",
        "#     # We need to adjust generation parameters for open-ended generation.\n",
        "#     # max_new_tokens controls how much new text is generated after the prompt.\n",
        "#     outputs = model_tinyllama.generate(\n",
        "#         inputs[\"input_ids\"],\n",
        "#         attention_mask=inputs[\"attention_mask\"],\n",
        "#         max_new_tokens=500,  # Generate up to 300 new tokens for the summary\n",
        "#         do_sample=True,     # Don't sample, use greedy decoding\n",
        "#         temperature=0.7,\n",
        "#         min_new_tokens=150,\n",
        "#         top_p=0.9,\n",
        "#         pad_token_id=tokenizer_tinyllama.eos_token_id, # Pad with EOS token if needed\n",
        "#     )\n",
        "\n",
        "#     # Decode the entire output sequence.\n",
        "#     generated_text = tokenizer_tinyllama.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "\n",
        "#     # The generated text will include the original prompt. We need to extract the summary part.\n",
        "#     # This is a simple approach, more sophisticated parsing might be needed depending on prompt and output.\n",
        "#     summary_start_index = generated_text.find(\"Summary:\") + len(\"Summary:\")\n",
        "#     summary = generated_text[summary_start_index:].strip()\n",
        "\n",
        "#     return summary\n",
        "\n",
        "# def answer_question_tinyllama(article, question):\n",
        "#     # Formulate the prompt to guide the TinyLlama model to answer the question based on the article.\n",
        "#     # We ask the model to act as an AI answering a question based on the provided text.\n",
        "#     input_text = f\"From this Article: {article}\\n\\n Answer the below Question: {question}\\n\\nAnswer:\"\n",
        "\n",
        "#     # Tokenize the input text\n",
        "#     # Truncate if the combined article and question is too long\n",
        "#     inputs = tokenizer_tinyllama(input_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
        "\n",
        "#     # Generate the answer using the model.\n",
        "#     # We use generate with parameters suitable for generating a concise answer.\n",
        "#     outputs = model_tinyllama.generate(\n",
        "#         inputs[\"input_ids\"],\n",
        "#         attention_mask=inputs[\"attention_mask\"],\n",
        "#         max_new_tokens=500,  # Generate up to 100 new tokens for the answer\n",
        "#         do_sample=True,      # Use sampling to potentially get more varied answers\n",
        "#         temperature=0.7,     # Control randomness\n",
        "#         top_p=0.9,           # Nucleus sampling\n",
        "#         pad_token_id=tokenizer_tinyllama.eos_token_id, # Pad with EOS token if needed\n",
        "#     )\n",
        "#     # Decode the generated sequence\n",
        "#     generated_text = tokenizer_tinyllama.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "#     # The generated text will include the original prompt. We need to extract the answer part.\n",
        "#     # This is a simple approach, more sophisticated parsing might be needed depending on prompt and output.\n",
        "#     answer_start_index = generated_text.find(\"Answer:\") + len(\"Answer:\")\n",
        "#     answer = generated_text[answer_start_index:].strip()\n",
        "\n",
        "#     # Basic cleanup: remove potential repetition of the question or prompt in the answer\n",
        "#     if answer.startswith(question):\n",
        "#         answer = answer[len(question):].strip()\n",
        "\n",
        "#     return answer\n",
        "\n",
        "\n",
        "# st.title(\"Smart Article Insights Generator\")\n",
        "# st.markdown(\"Summarize an article or ask a question about it.\")\n",
        "\n",
        "# mode = st.radio(\"Select Mode\", [\"Summarize\", \"Answer Question\"])\n",
        "\n",
        "# article_input = st.text_area(\"Article Text\", height=300, placeholder=\"Paste the article here...\")\n",
        "\n",
        "# question_input = None\n",
        "# if mode == \"Answer Question\":\n",
        "#     question_input = st.text_input(\"Question\", placeholder=\"Enter your question here...\")\n",
        "\n",
        "# if st.button(\"Process\"):\n",
        "#     if mode == \"Summarize\":\n",
        "#         if article_input:\n",
        "#             with st.spinner(\"Generating summary...\"):\n",
        "#                 output = summarize_tinyllama(article_input)\n",
        "#                 st.subheader(\"Summary\")\n",
        "#                 st.write(output)\n",
        "#         else:\n",
        "#             st.warning(\"Please provide an article to summarize.\")\n",
        "#     elif mode == \"Answer Question\":\n",
        "#         if article_input and question_input:\n",
        "#             with st.spinner(\"Generating answer...\"):\n",
        "#                 output = answer_question_tinyllama(article_input, question_input)\n",
        "#                 st.subheader(\"Answer\")\n",
        "#                 st.write(output)\n",
        "#         elif not article_input:\n",
        "#             st.warning(\"Please provide an article to answer the question from.\")\n",
        "#         elif not question_input:\n",
        "#             st.warning(\"Please provide a question to answer.\")\n"
      ],
      "metadata": {
        "id": "m_FsPjxIIZKa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25d1782d-2706-46fb-8734-96df8a0f25c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Docker File"
      ],
      "metadata": {
        "id": "17VcIdIcDadL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile Dockerfile\n",
        "\n",
        "# # Use an official Python runtime as a parent image\n",
        "# FROM python:3.10-slim\n",
        "\n",
        "\n",
        "# # Set environment variables\n",
        "# ENV PYTHONUNBUFFERED 1\n",
        "\n",
        "\n",
        "# # Install system dependencies and git\n",
        "# RUN apt-get update && apt-get install -y \\\n",
        "#     build-essential \\\n",
        "#     git \\\n",
        "#     && rm -rf /var/lib/apt/lists/*\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# # Create a non-root user and set permissions\n",
        "# RUN useradd -ms /bin/bash appuser\n",
        "# # Set the working directory in the container\n",
        "# WORKDIR /home/appuser/app\n",
        "\n",
        "\n",
        "# # Copy the requirements file and install dependencies\n",
        "# COPY requirements.txt .\n",
        "# RUN pip install --upgrade pip && pip install -r requirements.txt\n",
        "\n",
        "\n",
        "# # Switch to non-root user\n",
        "# USER appuser\n",
        "\n",
        "\n",
        "# # Copy the rest of the application code into the container\n",
        "# COPY --chown=appuser . /home/appuser/app\n",
        "\n",
        "\n",
        "# # Expose the port that the app runs on\n",
        "# EXPOSE 8501\n",
        "\n",
        "\n",
        "# # Command to run the application\n",
        "# CMD [\"streamlit\", \"run\", \"app.py\", \"--server.port=8501\", \"--server.address=0.0.0.0\"]\n",
        "\n"
      ],
      "metadata": {
        "id": "ASfiJU8kDXsP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "931ad5e4-f2a3-4f7e-d38d-698a3910b97b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing Dockerfile\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### requirements.txt"
      ],
      "metadata": {
        "id": "5DSuuldPKKId"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile requirements.txt\n",
        "# numpy==1.26.4\n",
        "# transformers==4.53.2\n",
        "# pandas==2.2.2\n",
        "# streamlit==1.47.0\n",
        "# torch==2.6.0\n",
        "# accelerate==1.9.0\n",
        "# huggingface_hub==0.33.4"
      ],
      "metadata": {
        "id": "R4lT85myKON8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5cda3ca-eea8-4f23-fae7-a1c1e51400be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing requirements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hugging Face Setup"
      ],
      "metadata": {
        "id": "LgOKkAclIi46"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Login to Hugging Face"
      ],
      "metadata": {
        "id": "7nTsv5ZqoH-z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Go to [Hugging face](https://huggingface.co) and sign up or log in to your account."
      ],
      "metadata": {
        "id": "um5a84WIoP2j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Create a New Space"
      ],
      "metadata": {
        "id": "MCzdrLYgoYsG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "   * Navigate to [Hugging face Spaces ](https://huggingface.co/spaces).\n",
        "   * Click **Create New Space**.\n",
        "   * Fill in:\n",
        "\n",
        "     * Name for your Space.\n",
        "     * Space SDK: Select **Docker**.\n",
        "     * Choose a Docker template: Select **Streamlit**\n",
        "     * Visibility: Choose *Public* or *Private*.\n",
        "   * Click **Create Space**."
      ],
      "metadata": {
        "id": "iReg7welocEh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Setup Hugging Face token\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "N_GU8gcMJ5e1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Open your Hugging Face Space\n",
        "\n",
        "- Click on the **â€œSettingsâ€** tab at the top of the Space.\n",
        "\n",
        "- Scroll down to the **â€œRepository secretsâ€** section.\n",
        "\n",
        "- Click **â€œAdd a new secretâ€**.\n",
        "\n",
        "- In the **Name** field, type `HF_TOKEN`\n",
        "\n",
        "- In the **Secret** field, paste your **HF token** (which you previously generated).\n",
        "\n",
        "- Click **â€œAdd secretâ€** to save it.\n"
      ],
      "metadata": {
        "id": "3p8iaRcqJwPk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Upload Your Files\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mrEhsirkos1w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "   * In the new Space, click the **Files** tab.\n",
        "   * Delete existing requirements.txt , DockerFile\n",
        "   * Click Contribute then Upload files and add:\n",
        "\n",
        "     * `app.py`\n",
        "     * `requirements.txt`\n",
        "     * `DockerFile`\n",
        "   * Commit the upload."
      ],
      "metadata": {
        "id": "8f2e8YFbo2Jg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Build and Launch\n",
        "\n"
      ],
      "metadata": {
        "id": "mMJf9s2WpMoi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "   * Hugging Face will automatically detect the `Dockerfile` and start building the container.\n",
        "   * Wait a few minutes for the build to complete and the app to go live."
      ],
      "metadata": {
        "id": "Z7MBsOPppdvk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Access and Test"
      ],
      "metadata": {
        "id": "nIHFm30QpluH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Go to the App tab or the Space URL to view and test your running Streamlit app.\n",
        "\n"
      ],
      "metadata": {
        "id": "d_hHgiFlpoN0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion"
      ],
      "metadata": {
        "id": "9rF3nbXbyAjZ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a81e12e5"
      },
      "source": [
        "* We developed a Smart Article Insights Generator to tackle information overload in the tech industry using transformer models such as FLAN-T5 (encoder-decoder) and TinyLlama (decoder-only).\n",
        "\n",
        "* FLAN-T5 faced limitations with long texts, while TinyLlama handled targeted queries well despite its small size and causal nature.\n",
        "\n",
        "* A Streamlit interface enables users to summarize articles or extract specific insights, supporting better decision-making.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Future Enhancements**\n",
        "\n",
        "\n",
        "* Lightweight base models like FLAN-T5 and TinyLlama have limited token capacity, which makes them struggle with long articles or multi-part tasks.\n",
        "  *Example*: When prompted with â€œSummarize this long article and also compare it with recent advancements in AI, in 100 words,â€ the model may skip key points or produce overly generic answers.\n",
        "\n",
        "* To tackle these issues, effective **prompt engineering** becomes essential. Breaking down complex queries into smaller, focused prompts can help guide the model more accurately and improve the relevance of its responses.\n",
        "\n",
        "\n",
        "* When dealing with large volumes of articles or documents exceeding token limits, models canâ€™t process all the content at once.\n",
        "  *Solution*: Future iterations will explore **RAG pipelines**, which retrieve only the most relevant chunks of information before generating a response  making the system scalable and accurate even with vast data.\n"
      ],
      "metadata": {
        "id": "dgxFHzq9Qfjy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font size=6 color=\"#4682B4\">Power Ahead!</font>\n",
        "___"
      ],
      "metadata": {
        "id": "26px7tU1C02z"
      }
    }
  ]
}