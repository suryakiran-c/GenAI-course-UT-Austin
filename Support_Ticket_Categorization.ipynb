{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmKLmjqmHMQM"
      },
      "source": [
        "<center><p float=\"center\">\n",
        "  <img src=\"https://upload.wikimedia.org/wikipedia/commons/e/e9/4_RGB_McCombs_School_Brand_Branded.png\" width=\"300\" height=\"100\"/>\n",
        "  <img src=\"https://mma.prnewswire.com/media/1458111/Great_Learning_Logo.jpg?p=facebook\" width=\"200\" height=\"100\"/>\n",
        "</p></center>\n",
        "\n",
        "<center><font size=10>Generative AI for Business Applications</center></font>\n",
        "<center><font size=6>Large Language Models & Prompt Engineering - Week 2</center></font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPYAUYpJ5MpY"
      },
      "source": [
        "<center><p float=\"center\">\n",
        "  <img src=\"https://cdn.pixabay.com/photo/2017/07/24/04/23/technical-support-2533526_1280.png\" width=\"480\"/>\n",
        "</p></center>\n",
        "\n",
        "<center><font size=5>Support Ticket Categorization</center></font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Bxnkoxf270y"
      },
      "source": [
        "# Problem Statement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PIXpOjx3DGc"
      },
      "source": [
        "## Business Context"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In today’s dynamic business landscape, organizations recognize the critical role of customer feedback in shaping products and services. Effectively leveraging this feedback enhances customer experiences, drives growth, and fosters long-term relationships. For Product Managers and Analysts, staying aligned with the voice of the customer is a strategic imperative.\n",
        "\n",
        "While organizations receive vast amounts of customer feedback and support tickets, the challenge lies in managing and utilizing this data effectively. A structured approach is essential, one that identifies key issues, prioritizes efficiently, and allocates resources wisely. Implementing a Support Ticket Categorization system is a powerful strategy to meet these needs; without it, teams may struggle to respond promptly to critical issues, leading to decreased customer satisfaction and engagement."
      ],
      "metadata": {
        "id": "x62rcEKC_B3l"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZyyo5p53BTG"
      },
      "source": [
        "## Objective"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The primary goals of the proposed support ticket categorization system are accurate classification, enabling a tagging mechanism, prioritization based on customer sentiment, and automated first response generation.\n",
        "\n",
        "The implementation of such a support ticket categorization system will empower the organization to respond proactively to customer feedback, ultimately leading to improved customer experiences and stronger, more enduring relationships with their client base, optimize resource allocation to address high-impact issues, and drive both growth and customer loyalty."
      ],
      "metadata": {
        "id": "jDktgZyA_LJG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Description"
      ],
      "metadata": {
        "id": "AAaZfw3ae5hc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset contains the following two columns:\n",
        "\n",
        "* **support\\_tick\\_id**: A unique identifier assigned to each support ticket.\n",
        "* **support\\_ticket\\_text**: The text content describing the issue reported in the support ticket."
      ],
      "metadata": {
        "id": "xqXhI8UAe-zj"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bonUQGu23RK"
      },
      "source": [
        "# Installing and Importing Necessary Libraries and Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "awsKQRs-OYo8"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers==4.53.2 \\\n",
        "                  accelerate==1.8.1 \\\n",
        "                  openai==1.96.1 \\\n",
        "                  bitsandbytes==0.46.1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note:\n",
        "- After running the above cell, kindly restart the runtime (for Google Colab) or notebook kernel (for Jupyter Notebook), and run all cells sequentially from the next cell.\n",
        "- On executing the above line of code, you might see a warning regarding package dependencies. This error message can be ignored as the above code ensures that all necessary libraries and their dependencies are maintained to successfully execute the code in this notebook."
      ],
      "metadata": {
        "id": "4EEnxPxFSCw-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Prompt:**\n",
        "\n",
        "<font size=3 color=\"#4682B4\"><b>I want to analyze the provided CSV data and work with AI models to understand the support tickets. Help me import the necessary Python libraries to:\n",
        "\n",
        "1. Read and manipulate the data</ul>\n",
        "2. Work with JSON data\n",
        "3. Working with system enviroment\n",
        "4. Connect to OpenAI models\n",
        "5. Use models from Hugging Face with AutoTokenizer and AutoModelForCausalLM\n",
        "\n",
        "</font>\n",
        "\n",
        "<font size=3 color=\"#4682B4\"><b>\n",
        "These libraries will help us load the data, connect with AI models, and prepare for further steps in the project.\n",
        "\n",
        "</font>"
      ],
      "metadata": {
        "id": "RM5RW5-0TopC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import os\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from openai import OpenAI"
      ],
      "metadata": {
        "id": "wdeAT_1EgI9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTpWESc53dL9"
      },
      "source": [
        "# Loading the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "er-hahR_HS7L"
      },
      "source": [
        "***Prompt***:\n",
        "\n",
        "<font size=3 color=\"#4682B4\"><b> Mount the Google Drive\n",
        "</font>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "yvmBPiheIm4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JhIr_HKrCN5"
      },
      "source": [
        "***Prompt***:\n",
        "\n",
        "<font size=3 color=\"#4682B4\"><b> Load the CSV file named \"support_ticket_data\" and store it in the variable data.\n",
        "</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ksv9hSCR4BM_"
      },
      "outputs": [],
      "source": [
        "# Load your CSV file\n",
        "data = pd.read_csv(\"/content/support_ticket_data.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8qUEOcQ3j5q"
      },
      "source": [
        "# Data Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23rEgLArjbuo"
      },
      "source": [
        "***Prompt***:\n",
        "\n",
        "<font size=3 color=\"#4682B4\"><b> Display the number of rows and columns in the `data`.\n",
        "</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4fLXRyDA4m3S"
      },
      "outputs": [],
      "source": [
        "data.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imUEo-l6na6D"
      },
      "source": [
        "* There are 21 rows and 2 columns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0J9f9vmyHir0"
      },
      "source": [
        "***Prompt***:\n",
        "\n",
        "<font size=3 color=\"#4682B4\"><b> Display the first 5 rows of the `data`.\n",
        "</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bNOTBqaNVG4j"
      },
      "outputs": [],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We create a copy of the original DataFrame to ensure that we always have the original support ticket data"
      ],
      "metadata": {
        "id": "8G_fHQto9Y_0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df=data.copy()"
      ],
      "metadata": {
        "id": "yue57kq49lOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qP5KTLo3OOC"
      },
      "source": [
        "# Model Loading"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll be using two language models:\n",
        "\n",
        "1. `mistralai/Mistral-7B-Instruct-v0.1` - an open-source model from Hugging Face.\n",
        "2. OpenAI's GPT model - accessed using an API key."
      ],
      "metadata": {
        "id": "jwG-gzl0-zd1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we start using these models, we need to set up and securely load our API tokens into the environment.  \n",
        "This ensures authenticated access to both Hugging Face and OpenAI services.\n"
      ],
      "metadata": {
        "id": "9jbb_RLK-3oM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting Up Hugging Face Token and OpenAI API Key"
      ],
      "metadata": {
        "id": "L7Kxp8iTj5LF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set Your OpenAI and Hugging face keys in the `config.json` file provided.\n",
        "\n",
        "1. Replace `\"your-hugging-face-token\"` with your actual Hugging Face key.\n",
        "2. Replace `\"your_api_key_here\"` with your actual OpenAI API key.\n",
        "3. Replace `\"your_base_url_here\"` with your OpenAI base URL\n",
        "\n"
      ],
      "metadata": {
        "id": "Xy8jodqHkuaw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Load from your config.json (update the path if needed)\n",
        "with open(\"Config.json\", \"r\") as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "# Extract the token\n",
        "HF_TOKEN = config.get('HF_TOKEN')\n",
        "OPENAI_API_KEY = config.get(\"OPENAI_API_KEY\")\n",
        "OPENAI_API_BASE = config.get(\"OPENAI_API_BASE\")"
      ],
      "metadata": {
        "id": "c5Fv9Jeyggwx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Mistral Model"
      ],
      "metadata": {
        "id": "cIfkyqocLm-I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE**\n",
        "\n",
        "1. We’re loading the entire model onto the local machine, which might take some time to initialize. To optimize this, we use 8-bit loading to reduce memory usage and speed up inference without significantly impacting performance.\n",
        "\n",
        "2. Before loading the model, you must first agree to its terms and conditions on Hugging Face. To do this, search for the model on the Hugging Face website, review its license or usage restrictions, and click “Agree and Access” to enable programmatic access via code.\n"
      ],
      "metadata": {
        "id": "EzRIuDrAS3lU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDjUfRIOzuWn"
      },
      "source": [
        "***Prompt***:\n",
        "\n",
        "<font size=3 color=\"#4682B4\"><b> Load the `mistralai/Mistral-7B-Instruct-v0.1` from hugging face using 8-bit quantization.\n",
        "\n",
        "</font>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "model_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    load_in_8bit=True,                 # Load the model with 8-bit quantization\n",
        "    torch_dtype=torch.float16,         # Use 16-bit floats on GPU\n",
        "    device_map=\"auto\",                 # Automatically assign GPU or CPU\n",
        "    token=HF_TOKEN                     # Hugging Face token for access\n",
        ")"
      ],
      "metadata": {
        "id": "xwBfPKrgn4VP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* `load_in_8bit=True`: Loads the model using 8-bit quantization to save memory.\n",
        "* `torch_dtype=torch.float16`: Uses half-precision (16-bit) floats for faster computation on GPU.\n",
        "* `device_map=\"auto\"`: Automatically places model layers across available devices.\n"
      ],
      "metadata": {
        "id": "8S5Ithq_EzCQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Hugging Face model is now ready. Let’s test it on an example input."
      ],
      "metadata": {
        "id": "_sR-dGKF_UBW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAsdu3G5nIOC"
      },
      "source": [
        "***Prompt***:\n",
        "\n",
        "<font size=3 color=\"#4682B4\"><b> Ask the Mistral model: What is the capital of France?\n",
        "</font>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the prompt (question)\n",
        "prompt = \"### Question: What is the capital of France?\\n### Answer:\"\n",
        "\n",
        "# Tokenize input\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "# Generate response\n",
        "outputs = model.generate(**inputs)\n",
        "\n",
        "# Decode and print the output\n",
        "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "edSsEJ5UsErS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, the model is returning results successfully."
      ],
      "metadata": {
        "id": "YG4KRgMA_kYC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s define a function that takes a `prompt` and a `query` as inputs and returns the model’s output.  \n",
        "\n",
        "- This will make it easier to reuse the model across different inputs."
      ],
      "metadata": {
        "id": "hefY6U1-_nH2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDCQOP1NnwaN"
      },
      "source": [
        "***Prompt***:\n",
        "\n",
        "<font size=3 color=\"#4682B4\"><b> Create a function that accepts a prompt and query, and returns the response generated by the Mistral model.\n",
        "</font>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def query_mistral(prompt, query):\n",
        "    \"\"\"\n",
        "    Queries the Mistral model with a given prompt and query.\n",
        "\n",
        "    Args:\n",
        "        prompt (str): The prompt for the model.\n",
        "        query (str): The query to be answered by the model.\n",
        "\n",
        "    Returns:\n",
        "        str: The model's response.\n",
        "    \"\"\"\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": prompt},\n",
        "        {\"role\": \"user\", \"content\": query}\n",
        "    ]\n",
        "    inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    pad_token_id = tokenizer.pad_token_id or tokenizer.eos_token_id\n",
        "\n",
        "    attention_mask = (inputs != pad_token_id).long()\n",
        "\n",
        "    outputs = model.generate(\n",
        "        inputs,\n",
        "        attention_mask=attention_mask,\n",
        "        max_new_tokens=100,  # Adjust as needed\n",
        "        do_sample=True,\n",
        "        temperature=0.7,     # Adjust as needed\n",
        "        top_p=0.9,           # Adjust as needed\n",
        "        pad_token_id=pad_token_id  # Prevents warning\n",
        "    )\n",
        "\n",
        "    # Decode and print the output, skipping the input tokens\n",
        "    response = tokenizer.decode(outputs[0][inputs.shape[-1]:], skip_special_tokens=True)\n",
        "    return response\n"
      ],
      "metadata": {
        "id": "UminawUZtUL3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the code snippet defined above, the following components are used:\n",
        "\n",
        "1. `tokenizer.apply_chat_template()`: This method converts the `messages` list into a single formatted string (e.g., adding special tokens or chat-style formatting), and tokenizes it into a tensor using PyTorch (`return_tensors=\"pt\"`). The `.to(model.device)` part ensures the tokenized input is moved to the same device as the model (like a GPU or CPU).\n",
        "\n",
        "2. `pad_token_id`: This variable is assigned the padding token ID used by the tokenizer. If the tokenizer does not explicitly define a `pad_token_id`, it falls back to the `eos_token_id` (end-of-sequence token). This is needed to handle padding properly during attention and generation.\n",
        "\n",
        "3. `attention_mask:` This creates a mask that tells the model which tokens should be attended to (represented by 1) and which should be ignored (usually padding tokens, represented by 0). It ensures the model focuses only on valid input tokens during processing.\n",
        "\n",
        "In the `generate()` function defined above, the following arguments are used:\n",
        "\n",
        "1. `max_new_tokens`: This parameter determines the maximum length of the generated sequence. In the provided code, max_new_tokens is set to 100, which means the generated sequence should not exceed 100 tokens.\n",
        "\n",
        "2. `temperature`: The temperature parameter controls the level of randomness in the generation process. A higher temperature (e.g., closer to 1) makes the output more diverse and creative but potentially less focused, while a lower temperature (e.g., close to 0) produces more deterministic and focused but potentially repetitive outputs. In the code, temperature is set to 0.7, indicating a very low temperature and, consequently, a more deterministic sampling.\n",
        "\n",
        "3. `do_sample`: This is a boolean parameter that determines whether to use sampling during generation (do_sample=True) or use greedy decoding (do_sample=False). When set to True, as in the provided code, the model samples from the distribution of predicted tokens at each step, introducing randomness in the generation process.\n",
        "\n",
        "4. `top_p`: Controls how many top probable tokens to consider during generation. If set to 0.9, it samples from the smallest set of tokens whose combined probability is at least 90%, balancing creativity and coherence.\n"
      ],
      "metadata": {
        "id": "18d1tmR1mVqw"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWvf3R3An5K4"
      },
      "source": [
        "## Loading OpenAI model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbsa0OGPoFLJ"
      },
      "source": [
        "***Prompt***:\n",
        "\n",
        "<font size=3 color=\"#4682B4\"><b> Create an OpenAI client\n",
        "</font>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "openai_client = OpenAI(api_key=OPENAI_API_KEY, base_url=OPENAI_API_BASE)"
      ],
      "metadata": {
        "id": "fSsdWE74oLR1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OpenAI model is now ready. Let’s test it on an example input."
      ],
      "metadata": {
        "id": "Wfff9Bpw_yHR"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZAPl1pgoreW"
      },
      "source": [
        "***Prompt***:\n",
        "\n",
        "<font size=3 color=\"#4682B4\"><b> Ask the OpenAI model: What is the capital of France?\n",
        "</font>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Ask the OpenAI model: What is the capital of France?\n",
        "\n",
        "query = \"What is the capital of France?\"\n",
        "\n",
        "response = openai_client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",  # Or another suitable OpenAI model\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": query}\n",
        "    ],\n",
        "    max_tokens=100\n",
        ")\n",
        "\n",
        "response.choices[0].message.content\n"
      ],
      "metadata": {
        "id": "TyNtDmZtoTRC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model is returning results successfully."
      ],
      "metadata": {
        "id": "eoF1jl23_t7V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, let’s define a function that takes a `prompt` and a `query` as inputs and returns the model’s output."
      ],
      "metadata": {
        "id": "rBtMvtIk_t7V"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYUZqUIgotcY"
      },
      "source": [
        "***Prompt***:\n",
        "\n",
        "<font size=3 color=\"#4682B4\"><b> Create a function that accepts a prompt and query, and returns the response generated by the OpenAI model.\n",
        "</font>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Create a function that accepts a prompt and query, and returns the response generated by the OpenAI model.\n",
        "\n",
        "def query_openai(prompt, query):\n",
        "    \"\"\"\n",
        "    Queries the OpenAI model with a given prompt and query.\n",
        "\n",
        "    Args:\n",
        "        prompt (str): The prompt for the model.\n",
        "        query (str): The query to be answered by the model.\n",
        "\n",
        "    Returns:\n",
        "        str: The model's response.\n",
        "    \"\"\"\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": prompt},\n",
        "        {\"role\": \"user\", \"content\": query}\n",
        "    ]\n",
        "    response = openai_client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",  # Or another suitable OpenAI model\n",
        "        messages=messages,\n",
        "        max_tokens=500  # Adjust max_tokens as needed\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# Example usage:\n",
        "# prompt_text = \"You are a helpful assistant.\"\n",
        "# query_text = \"Explain the process of ticket categorization.\"\n",
        "# openai_response = query_openai(prompt_text, query_text)\n",
        "# print(openai_response)\n"
      ],
      "metadata": {
        "id": "J6Mo3U1EokSS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ticket Categorization and Response Generation"
      ],
      "metadata": {
        "id": "lc49MXeEo_pY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s take a sample from the support tickets to observe how the model performs on each task.\n"
      ],
      "metadata": {
        "id": "Wprva8z-AEem"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YumXLnrlAW2_"
      },
      "source": [
        "***Prompt***:\n",
        "\n",
        "<font size=3 color=\"#4682B4\"><b>Take the first \"support_ticket_text\" from the data and store it in the variable sample_ticket.\n",
        "</font>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_ticket=data['support_ticket_text'][0]"
      ],
      "metadata": {
        "id": "agbWvyhYpGgn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "le80Djip27mc"
      },
      "source": [
        "## Task 1: Ticket Categorization using Zero Shot Prompting"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our first task is to classify the support ticket into a predefined category. We will write a prompt that instructs the model to return the category in **JSON format**, so that we receive the output as structured data that’s easy to parse and store."
      ],
      "metadata": {
        "id": "zaI6ed21A-xF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classification_prompt = \"\"\"\n",
        " You are a technical assistant. Classify the support ticket based on the Support Ticket Text presented in the input into the following categories and not any other.\n",
        "    - Technical issues\n",
        "    - Hardware issues\n",
        "    - Data recovery\n",
        "Return only a structured JSON output in the following format:\n",
        "{\"Category\": \"category_prediction\"}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "yPIFcGW3x-_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using Mistral"
      ],
      "metadata": {
        "id": "UfqBaEwD3FNh"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSSp4w-O3FNh"
      },
      "source": [
        "***Prompt***:\n",
        "\n",
        "<font size=3 color=\"#4682B4\"><b> Define a function named classify_ticket_mistral that takes the classification prompt and the support ticket text as input, gets the result from query_mistral function, and returns the result in a JSON format.\n",
        "</font>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_ticket_mistral(prompt, query):\n",
        "    \"\"\"\n",
        "    Classifies a support ticket using the OpenAI model and returns the result in JSON format.\n",
        "\n",
        "    Args:\n",
        "        prompt (str): The classification prompt for the model.\n",
        "        query (str): The support ticket text to be classified.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the classification result, or None if classification fails.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response_text = query_mistral(prompt, query)\n",
        "        # Attempt to parse the response text as JSON\n",
        "        classification_result = json.loads(response_text)\n",
        "        return classification_result\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"Error decoding JSON from OpenAI response: {e}\")\n",
        "        print(f\"Raw OpenAI response: {response_text}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "        return None\n",
        "\n"
      ],
      "metadata": {
        "id": "2iv3KecC3FNi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classify=classify_ticket_mistral(classification_prompt,sample_ticket)\n",
        "print(sample_ticket)\n",
        "print(classify)"
      ],
      "metadata": {
        "id": "5SIgRYtY3FNi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Ticket text : My internet connection has significantly slowed down over the past two days, making it challenging to work efficiently from home. Frequent disconnections are causing major disruptions. Please assist in resolving this connectivity issue promptly.\n",
        "\n",
        "> Category : Technical Issue"
      ],
      "metadata": {
        "id": "Vy6UkNOr3FNj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using OpenAI"
      ],
      "metadata": {
        "id": "sBrB_F85223i"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lcVobKup7SJ"
      },
      "source": [
        "***Prompt***:\n",
        "\n",
        "<font size=3 color=\"#4682B4\"><b> Define a function named classify_ticket_openai that takes the classification prompt and the support ticket text as input, gets the result from query_openai function, and returns the result in a JSON format.\n",
        "</font>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_ticket_openai(prompt, query):\n",
        "    \"\"\"\n",
        "    Classifies a support ticket using the OpenAI model and returns the result in JSON format.\n",
        "\n",
        "    Args:\n",
        "        prompt (str): The classification prompt for the model.\n",
        "        query (str): The support ticket text to be classified.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the classification result, or None if classification fails.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response_text = query_openai(prompt, query)\n",
        "        # Attempt to parse the response text as JSON\n",
        "        classification_result = json.loads(response_text)\n",
        "        return classification_result\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"Error decoding JSON from OpenAI response: {e}\")\n",
        "        print(f\"Raw OpenAI response: {response_text}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "        return None\n",
        "\n"
      ],
      "metadata": {
        "id": "NUhcgUvSpNAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classify=classify_ticket_openai(classification_prompt,sample_ticket)\n",
        "print(sample_ticket)\n",
        "print(classify)"
      ],
      "metadata": {
        "id": "vOHKXNImyaDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Ticket Text : My internet connection has significantly slowed down over the past two days, making it challenging to work efficiently from home. Frequent disconnections are causing major disruptions. Please assist in resolving this connectivity issue promptly.\n",
        "\n",
        "> Category : Technical Issue"
      ],
      "metadata": {
        "id": "GxdQtSqJRORK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Both models generate results in the desired format. However, we'll proceed with OpenAI, as its API-based setup provides slightly faster responses."
      ],
      "metadata": {
        "id": "KGm3fOxoBixV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlpGNtUyrAhN"
      },
      "source": [
        "***Prompt***:\n",
        "\n",
        "<font size=3 color=\"#4682B4\"><b>Generate the category for each support_ticket_text in the DataFrame using the classify_ticket_openai function, and store the result in a new column.\n",
        "\n",
        "</font>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the classification function to each row in the DataFrame\n",
        "df['Category'] = df['support_ticket_text'].apply(lambda x: classify_ticket_openai(classification_prompt, x)['Category'] if classify_ticket_openai(classification_prompt, x) else None)"
      ],
      "metadata": {
        "id": "W0mIwZjgq24j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "LNThjS9My8Ax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z795llV0elBQ"
      },
      "source": [
        "## Task 2: Creating Tags using Few Shot Prompting"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this task, we will use few-shot prompting instead of zero-shot prompting because it provides the model with concrete examples, helping it understand how to extract and structure the information accurately.\n",
        "- Zero-shot prompting may result in inconsistent formats, incorrect category\n",
        "mapping, or misinterpretation of impact levels due to a lack of context and guidance."
      ],
      "metadata": {
        "id": "7_9oOTqGodMp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metadata_prompt = \"\"\"\n",
        "You are an intelligent assistant that extracts structured metadata from technical support queries.\n",
        "Analyze the query and extract the following information:\n",
        "\n",
        "* Device (e.g., Laptop, Phone, Router, etc.)\n",
        "* Problem Type (e.g., Not Turning On, Lost Internet, Deleted Files)\n",
        "* User Impact - Estimate based on how severely the issue affects the user's ability to continue working or using the device:\n",
        "\n",
        "    - * Major: The user cannot proceed with work at all.\n",
        "    - * Moderate: The user is impacted but may have a workaround.\n",
        "    - * Minor: The issue is present but does not significantly hinder usage.\n",
        "\n",
        "Use the following examples as guidance.\n",
        "\n",
        "Query Text: My phone battery is draining rapidly even on battery saver mode. I barely use it and it drops 50% in a few hours.\n",
        "Output: {\"Device\": \"Phone\", \"Problem Type\": \"Battery Draining\", \"User Impact\": \"Minor\"}\n",
        "\n",
        "Query Text: I accidentally deleted a folder containing all project files. Please help me recover it.\n",
        "Output: {\"Device\": \"Laptop\", \"Problem Type\": \"Deleted Files\", \"User Impact\": \"Major\"}\n",
        "\n",
        "Query Text: My router is not working.\n",
        "Output: {\"Device\": \"Router\", \"Problem Type\": \"Lost Internet\", \"User Impact\": \"Moderate\"}\n",
        "\n",
        "Return the final output only in a valid JSON format without any additional explanation.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "I-8JVxT7ryQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using Mistral"
      ],
      "metadata": {
        "id": "RO7H34JN34vK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzjzYDIv34vK"
      },
      "source": [
        "***Prompt***:\n",
        "\n",
        "<font size=3 color=\"#4682B4\"><b> Define a function named extract_metadata_mistral that takes the metadata prompt and query as input get the result from query_mistral function return the result in JSON format.\n",
        "</font>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_metadata_mistral(prompt, query):\n",
        "    \"\"\"\n",
        "    Extracts metadata from a support ticket using the OpenAI model and returns the result in JSON format.\n",
        "\n",
        "    Args:\n",
        "        prompt (str): The metadata extraction prompt for the model.\n",
        "        query (str): The support ticket text to extract metadata from.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the extracted metadata, or None if extraction fails.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response_text = query_mistral(prompt, query)\n",
        "        # Attempt to parse the response text as JSON\n",
        "        metadata_result = json.loads(response_text)\n",
        "        return metadata_result\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"Error decoding JSON from OpenAI response: {e}\")\n",
        "        print(f\"Raw OpenAI response: {response_text}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "xcUXwDG134vK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metadata=extract_metadata_mistral(metadata_prompt,sample_ticket)\n",
        "print(sample_ticket)\n",
        "print(metadata)"
      ],
      "metadata": {
        "id": "d0-9a5qz34vL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Ticket text : My internet connection has significantly slowed down over the past two days, making it challenging to work efficiently from home. Frequent disconnections are causing major disruptions. Please assist in resolving this connectivity issue promptly.\n",
        "\n",
        "> Category : Technical Issue\n",
        "\n",
        "> Device : Laptop\n",
        "\n",
        "> Problem Type : Lost Internet\n",
        "\n",
        "> User Impace : Major"
      ],
      "metadata": {
        "id": "X3GL2I7H34vL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation:**\n",
        "\n",
        "* The issue described is \"Slowed Internet\" due to frequent disconnections, not a complete loss; the problem likely lies with the router, not the laptop.\n",
        "* The model incorrectly tagged the **Device** as \"Laptop\" and **Problem Type** as \"Lost Internet.\"\n",
        "\n",
        "Let's try to generate metadata using OpenAI.\n"
      ],
      "metadata": {
        "id": "cKmYACL64p9g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using Openai"
      ],
      "metadata": {
        "id": "2gVTYch-30qW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2kQLdXPrPL-"
      },
      "source": [
        "***Prompt***:\n",
        "\n",
        "<font size=3 color=\"#4682B4\"><b> Define a function named extract_metadata_openai that takes the metadata prompt and query as input get the result from query_openai function return the result in JSON format.\n",
        "</font>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_metadata_openai(prompt, query):\n",
        "    \"\"\"\n",
        "    Extracts metadata from a support ticket using the OpenAI model and returns the result in JSON format.\n",
        "\n",
        "    Args:\n",
        "        prompt (str): The metadata extraction prompt for the model.\n",
        "        query (str): The support ticket text to extract metadata from.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the extracted metadata, or None if extraction fails.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response_text = query_openai(prompt, query)\n",
        "        # Attempt to parse the response text as JSON\n",
        "        metadata_result = json.loads(response_text)\n",
        "        return metadata_result\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"Error decoding JSON from OpenAI response: {e}\")\n",
        "        print(f\"Raw OpenAI response: {response_text}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "0P66XewvrXE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metadata=extract_metadata_openai(metadata_prompt,sample_ticket)\n",
        "print(sample_ticket)\n",
        "print(metadata)"
      ],
      "metadata": {
        "id": "vAN_KmJazOEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Ticket text : My internet connection has significantly slowed down over the past two days, making it challenging to work efficiently from home. Frequent disconnections are causing major disruptions. Please assist in resolving this connectivity issue promptly.\n",
        "\n",
        "> Category : Technical Issue\n",
        "\n",
        "> Device : Router\n",
        "\n",
        "> Problem Type : Slow Internet and Frequent Disconnections\n",
        "\n",
        "> User Impace : Major"
      ],
      "metadata": {
        "id": "zETDRec3RbyD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation:**\n",
        "\n",
        "* OpenAI is generating the correct metadata based on the ticket description.\n",
        "* Therefore, we will proceed with OpenAI for further ticket categorization."
      ],
      "metadata": {
        "id": "QTSVHgbQ5Bdm"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXM1xt1Tr-fH"
      },
      "source": [
        "***Prompt***:\n",
        "\n",
        "<font size=3 color=\"#4682B4\"><b> Generate the metadata for each support_ticket_text in the DataFrame using extract_metadata_openai, store it in a new column, and extract individual fields into separate columns.\n",
        "</font>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate metadata for each support_ticket_text and store it in a new column\n",
        "df['Metadata'] = df['support_ticket_text'].apply(lambda x: extract_metadata_openai(metadata_prompt, x))\n",
        "\n",
        "# Extract individual metadata fields into separate columns\n",
        "df['Device'] = df['Metadata'].apply(lambda x: x.get('Device') if x else None)\n",
        "df['Problem Type'] = df['Metadata'].apply(lambda x: x.get('Problem Type') if x else None)\n",
        "df['User Impact'] = df['Metadata'].apply(lambda x: x.get('User Impact') if x else None)"
      ],
      "metadata": {
        "id": "GOgH1IjXrrU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "Zshqw5GjPi8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdwE7rguh8sD"
      },
      "source": [
        "## Task 3: Predicting Priority using Chain of Thought Prompting"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this task, Chain-of-Thought (CoT) prompting is crucial because assigning support ticket priority involves understanding several subtle cues like urgency, usability, and user sentiment.\n",
        "\n",
        "- CoT allows the model to reason step-by-step through these factors, leading to\n",
        "more accurate and context-aware decisions. Without this, zero-shot prompts may miss key details and result in incorrect or inconsistent priority levels."
      ],
      "metadata": {
        "id": "IKshedOPpLDK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "priority_prompt =\"\"\"\n",
        "You are an intelligent assistant that determines the priority level of a support ticket.\n",
        "\n",
        "For any given ticket, follow this step-by-step reasoning process to assign the correct priority level: Low, Medium, High.\n",
        "\n",
        "Step-by-step Evaluation:\n",
        "\n",
        "Is the device or service completely unusable?\n",
        "\n",
        "Is the issue blocking critical or time-sensitive work?\n",
        "\n",
        "Is there a specific deadline or urgency mentioned by the user?\n",
        "\n",
        "Does the user mention partial functionality or ongoing work?\n",
        "\n",
        "Is the tone or language expressing frustration or emergency?\n",
        "\n",
        "After evaluating all the above steps, decide the most appropriate priority level based on the impact and urgency.\n",
        "\n",
        "Return the final output only in a valid JSON format as mentioned below without any additional explanation:\n",
        "{\"priority\": \"High\"}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "wGXJ_lGpz0H-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLMl9uH2tGUm"
      },
      "source": [
        "***Prompt***:\n",
        "\n",
        "<font size=3 color=\"#4682B4\"><b> Define a function that takes the priority prompt, problem type, user Impact, and query as input get the result from query_openai function return the result in JSON format.\n",
        "</font>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_priority(prompt, query, problem_type, user_impact):\n",
        "    \"\"\"\n",
        "    Predicts the priority of a support ticket using the OpenAI model and returns the result in JSON format.\n",
        "\n",
        "    Args:\n",
        "        prompt (str): The priority prediction prompt for the model.\n",
        "        query (str): The support ticket text to predict the priority for.\n",
        "        problem_type (str): The extracted problem type.\n",
        "        user_impact (str): The extracted user impact.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the predicted priority, or None if prediction fails.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Include problem_type and user_impact in the query sent to the model\n",
        "        full_query = f\"\"\"\n",
        "        Support Ticket: {query}\n",
        "        Problem Type: {problem_type}\n",
        "        User Impact: {user_impact}\n",
        "\n",
        "        Based on the support ticket, problem type, and user impact, predict the priority: Low, Medium, High.\n",
        "        Return only a structured JSON output in the following format:\n",
        "        {{\"priority\": \"priority_prediction\"}}\n",
        "        \"\"\"\n",
        "        response_text = query_openai(prompt, full_query)\n",
        "        priority_result = json.loads(response_text)\n",
        "        return priority_result\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"Error decoding JSON from OpenAI response: {e}\")\n",
        "        print(f\"Raw OpenAI response: {response_text}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "z62s6umCtJd8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "priority = predict_priority(priority_prompt, sample_ticket,metadata['Problem Type'],metadata['User Impact'])\n",
        "print(sample_ticket)\n",
        "priority"
      ],
      "metadata": {
        "id": "2JyzeYNP0EV2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Ticket text : My internet connection has significantly slowed down over the past two days, making it challenging to work efficiently from home. Frequent disconnections are causing major disruptions. Please assist in resolving this connectivity issue promptly.\n",
        "\n",
        "> Category : Technical Issue\n",
        "\n",
        "> Device : Router\n",
        "\n",
        "> Problem Type : Slow Internet Connection\n",
        "\n",
        "> User Impact : Major\n",
        "\n",
        "> Priority: High"
      ],
      "metadata": {
        "id": "EEstjB8iT9tt"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZZmaaf9ugxV"
      },
      "source": [
        "***Prompt***:\n",
        "\n",
        "<font size=3 color=\"#4682B4\"><b> Generate the priotity for each support_ticket_text in the df and store it in a new column.\n",
        "</font>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Generate the priotity for each support_ticket_text in the df and store it in a new column\n",
        "# Generate the priority for each support_ticket_text and store it in a new column\n",
        "df['Priority'] = df.apply(lambda row: predict_priority(priority_prompt, row['support_ticket_text'], row['Problem Type'], row['User Impact'])['priority'] if predict_priority(priority_prompt, row['support_ticket_text'], row['Problem Type'], row['User Impact']) else None, axis=1)"
      ],
      "metadata": {
        "id": "sLpVJiwnul8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "FMNqIujH2seq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7m73qRcne4g"
      },
      "source": [
        "## Task 4 - Creating a Draft Response using Chain of Thought Prompting"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will continue using Chain-of-Thought (CoT) prompting for this task because generating a thoughtful, empathetic response requires the model to reason through multiple elements, such as the user's concern, the issue type, priority-based urgency, and appropriate tone.\n",
        "- CoT helps the model integrate all these cues step-by-step to produce a relevant and human-like reply."
      ],
      "metadata": {
        "id": "VAroVSTOpZEe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1CYnMUZUnqkf"
      },
      "outputs": [],
      "source": [
        "response_prompt = \"\"\"\n",
        "You are provided with a support ticket's text along with its Category, Tags, and assigned Priority level.\n",
        "\n",
        "Follow these steps before generating your final response:\n",
        "\n",
        "1. Analyze the ticket text to understand the customer's sentiment and main concern.\n",
        "2. Identify the issue type using the provided Category and Tags.\n",
        "3. Determine the appropriate ETA based on the Priority level.\n",
        "4. Compose a short, empathetic response that reassures the customer, acknowledges their concern, and includes the ETA.\n",
        "\n",
        "Ensure the final response:\n",
        "\n",
        "1. Is under 50 words\n",
        "2. Has a polite and empathetic tone\n",
        "3. Addresses the issue clearly\n",
        "\n",
        "Return only the final response to the customer. Do not include your reasoning steps in the output.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3SWGDv2vLAc"
      },
      "source": [
        "***Prompt***:\n",
        "\n",
        "<font size=3 color=\"#4682B4\"><b> Define a function that takes the `response_prompt`, `query`, `category`, `metadata_tags`, and `priority` as inputs. Pass `response_prompt` as the system prompt, and combine the remaining inputs (`query`, `category`, `metadata_tags`, and `priority`) into a single message. Pass this message to the `query_openai` function and return the result.\n",
        "</font>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Define a function that takes response_prompt, query, category, metadata_tags, and priority as inputs. Pass response_prompt as the system prompt, and combine the remaining inputs (query, category, metadata_tags, and priority) into a single message. Pass this message to the query_openai function and return the result.\n",
        "def generate_response(response_prompt, query, category, metadata_tags, priority):\n",
        "    \"\"\"\n",
        "    Generates a draft response for a support ticket using the OpenAI model.\n",
        "\n",
        "    Args:\n",
        "        response_prompt (str): The prompt for generating the response.\n",
        "        query (str): The original support ticket text.\n",
        "        category (str): The predicted category of the ticket.\n",
        "        metadata_tags (dict): The extracted metadata tags (Device, Problem Type, User Impact).\n",
        "        priority (str): The predicted priority of the ticket.\n",
        "\n",
        "    Returns:\n",
        "        str: The generated response text, or None if response generation fails.\n",
        "    \"\"\"\n",
        "    # Combine the inputs into a single message for the model\n",
        "    user_message = f\"\"\"\n",
        "    Support Ticket: {query}\n",
        "    Category: {category}\n",
        "    Metadata Tags: {metadata_tags}\n",
        "    Priority: {priority}\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        # Pass the combined message to the query_openai function\n",
        "        response_text = query_openai(response_prompt, user_message)\n",
        "        return response_text\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred during response generation: {e}\")\n",
        "        return None\n",
        "\n",
        "# Example usage (assuming you have values for query, category, metadata_tags, priority):\n",
        "# sample_query = df['support_ticket_text'][0]\n",
        "# sample_category = df['Category'][0]\n",
        "# sample_metadata_tags = df['Metadata'][0]\n",
        "# sample_priority = df['Priority'][0]\n",
        "\n",
        "# draft_response = generate_response(response_prompt, sample_query, sample_category, sample_metadata_tags, sample_priority)\n",
        "# print(draft_response)"
      ],
      "metadata": {
        "id": "zmIo_dmivifl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_query = df['support_ticket_text'][0]\n",
        "sample_category = df['Category'][0]\n",
        "sample_metadata_tags = df['Metadata'][0]\n",
        "sample_priority = df['Priority'][0]\n",
        "\n",
        "draft_response = generate_response(response_prompt, sample_query, sample_category, sample_metadata_tags, sample_priority)\n",
        "print(draft_response)"
      ],
      "metadata": {
        "id": "3LDDAsfH0f8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Ticket text : My internet connection has significantly slowed down over the past two days, making it challenging to work efficiently from home. Frequent disconnections are causing major disruptions. Please assist in resolving this connectivity issue promptly.\n",
        "\n",
        "> Category : Technical Issue\n",
        "\n",
        "> Device : Router\n",
        "\n",
        "> Problem Type : Slow Internet Connection\n",
        "\n",
        "> User Impact : Major\n",
        "\n",
        "> Priority : High\n",
        "\n",
        "> Response : I understand the urgency of your situation. Our team will prioritize resolving your slow internet and disconnection issues. We aim to have this addressed within the next 24 hours to improve your work efficiency. Thank you for your patience.\n"
      ],
      "metadata": {
        "id": "JnRfOCRKWs_V"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "miyKuq5WwtUv"
      },
      "source": [
        "***Prompt***:\n",
        "\n",
        "<font size=3 color=\"#4682B4\"><b> Generate the draft_response for each support_ticket_text in the df and store it in a new column.\n",
        "</font>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['draft_response'] = df.apply(lambda row: generate_response(response_prompt, row['support_ticket_text'], row['Category'], row['Metadata'], row['Priority']), axis=1)"
      ],
      "metadata": {
        "id": "PkAhxKF7xBcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "XyVE0w05-gfW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reviewing Model Output"
      ],
      "metadata": {
        "id": "07Fny78vk063"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have converted all the support tickets into structured data, let's review a few tickets along with their classifications.\n"
      ],
      "metadata": {
        "id": "_Snk7Nu1jDvd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Prompt***:\n",
        "\n",
        "<font size=3 color=\"#4682B4\"><b> Create a function that takes a support ticket ID as input and displays the details extracted from the model on a separate line.\n",
        "</font>"
      ],
      "metadata": {
        "id": "F4VJLk9MtQiX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def display_ticket_details(ticket_id, dataframe):\n",
        "  \"\"\"\n",
        "  Displays all details of a support ticket from a DataFrame, with each detail on a new line.\n",
        "\n",
        "  Args:\n",
        "    ticket_id (str): The ID of the support ticket to display.\n",
        "    dataframe (pd.DataFrame): The DataFrame containing the support ticket data.\n",
        "  \"\"\"\n",
        "  ticket_details = dataframe[dataframe['support_tick_id'] == ticket_id]\n",
        "\n",
        "  if not ticket_details.empty:\n",
        "    for column in ticket_details.columns:\n",
        "      print(f\"{column}: {ticket_details.iloc[0][column]}\")\n",
        "  else:\n",
        "    print(f\"Ticket with ID {ticket_id} not found.\")"
      ],
      "metadata": {
        "id": "GgkjRP3PjEzZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ticket ID: ST2023-009"
      ],
      "metadata": {
        "id": "JsJd2se4kUDK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage:\n",
        "display_ticket_details('ST2023-009', df)"
      ],
      "metadata": {
        "id": "yKUmv15KkPmT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations:**\n",
        "\n",
        "- The system correctly identifies the device as *Router* and the problem type as *Weak Wi-Fi Signal*, matching the ticket description.\n",
        "- The impact is rightly marked as *Moderate*, as the issue is persistent and affects usability.\n",
        "- The draft response is polite, empathetic, and includes a clear resolution timeline, which helps manage user expectations.\n"
      ],
      "metadata": {
        "id": "ZdvAQcq159mh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ticket ID: ST2023-018"
      ],
      "metadata": {
        "id": "oGv_lU9mkntt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage:\n",
        "display_ticket_details('ST2023-018', df)"
      ],
      "metadata": {
        "id": "LsO1z03Hkntu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations:**\n",
        "\n",
        "- The system accurately detects the *Device* as **Laptop** and the *Problem Type* as **Liquid Damage**, aligning with the ticket text.\n",
        "- The *User Impact* is rightly set as **Major**, given the device won’t turn on, and *Priority* is appropriately marked as **High**.\n",
        "- The draft response is empathetic and assures immediate action with a ETA 2-3 days, which builds user trust.\n"
      ],
      "metadata": {
        "id": "bP96ZOd36Hf_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ticket ID: ST2023-023"
      ],
      "metadata": {
        "id": "07i8IrXkkWMb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage:\n",
        "display_ticket_details('ST2023-023', df)"
      ],
      "metadata": {
        "id": "geb4mxwhkbPv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations:**\n",
        "\n",
        "- The *Device* is correctly identified as **USB Drive**, and the *Problem Type* as **Formatted Drive**, matching the user’s concern.\n",
        "- The *User Impact* is rightly marked **Major** due to potential data loss, with *Priority* set to **High**.\n",
        "- The draft reply is empathetic and ensures quick action with a clear resolution timeline of **24 hours**, reassuring the user.\n"
      ],
      "metadata": {
        "id": "51unCVOq6Rcp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deployment"
      ],
      "metadata": {
        "id": "Zmr3wnbT_jAX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we deploy the system using Hugging Face Spaces with Streamlit.\n",
        "\n",
        "> **Note:** This is purely for demonstration purposes, so we won't dive deep into the code here."
      ],
      "metadata": {
        "id": "SXKQbrnoMmxX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Streamlit on Hugging Face"
      ],
      "metadata": {
        "id": "yHKUaar2KtDi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### app.py"
      ],
      "metadata": {
        "id": "l54U7FCxDnDl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE**: You can use this prompt to generate the code for buidling the Streamlit app. However, a few minor adjustments might be needed afterwards."
      ],
      "metadata": {
        "id": "WKWjfsLj8mbR"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3sEOz9O8X2q"
      },
      "source": [
        "***Prompt***:\n",
        "\n",
        "<font size=3 color=\"#4682B4\"><b> Create a complete Streamlit app that includes all required functions and prompts, uses the OpenAI API to fetch results, and displays all the final outputs at the end.\n",
        "</font>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile app.py\n",
        "\n",
        "# import streamlit as st\n",
        "# import pandas as pd\n",
        "# import json\n",
        "# import os\n",
        "# from openai import OpenAI\n",
        "\n",
        "# # Load OpenAI API key and base URL from Colab secrets\n",
        "# try:\n",
        "#     OPENAI_API_KEY = os.environ.get(\"API_KEY\")\n",
        "#     OPENAI_API_BASE = os.environ.get(\"API_BASE\")\n",
        "#     openai_client = OpenAI(api_key=OPENAI_API_KEY, base_url=OPENAI_API_BASE)\n",
        "# except Exception as e:\n",
        "#     st.error(f\"Error loading OpenAI credentials: {e}\")\n",
        "#     st.stop()\n",
        "\n",
        "\n",
        "# # Define the functions for categorization, metadata extraction, priority prediction, and response generation\n",
        "# def query_openai(prompt, query):\n",
        "#     \"\"\"\n",
        "#     Queries the OpenAI model with a given prompt and query.\n",
        "#     Args:\n",
        "#         prompt (str): The prompt for the model.\n",
        "#         query (str): The query to be answered by the model.\n",
        "#     Returns:\n",
        "#         str: The model's response.\n",
        "#     \"\"\"\n",
        "#     messages = [\n",
        "#         {\"role\": \"system\", \"content\": prompt},\n",
        "#         {\"role\": \"user\", \"content\": query}\n",
        "#     ]\n",
        "#     response = openai_client.chat.completions.create(\n",
        "#         model=\"gpt-3.5-turbo\",  # Or another suitable OpenAI model\n",
        "#         messages=messages,\n",
        "#         max_tokens=100  # Adjust max_tokens as needed\n",
        "#     )\n",
        "#     return response.choices[0].message.content\n",
        "\n",
        "# def classify_ticket(prompt, query):\n",
        "#     \"\"\"\n",
        "#     Classifies a support ticket using the OpenAI model and returns the result in JSON format.\n",
        "#     Args:\n",
        "#         prompt (str): The classification prompt for the model.\n",
        "#         query (str): The support ticket text to be classified.\n",
        "#     Returns:\n",
        "#         dict: A dictionary containing the classification result, or None if classification fails.\n",
        "#     \"\"\"\n",
        "#     try:\n",
        "#         response_text = query_openai(prompt, query)\n",
        "#         # Attempt to parse the response text as JSON\n",
        "#         classification_result = json.loads(response_text)\n",
        "#         return classification_result\n",
        "#     except json.JSONDecodeError as e:\n",
        "#         st.error(f\"Error decoding JSON from OpenAI response: {e}\")\n",
        "#         st.text(f\"Raw OpenAI response: {response_text}\")\n",
        "#         return None\n",
        "#     except Exception as e:\n",
        "#         st.error(f\"An unexpected error occurred during classification: {e}\")\n",
        "#         return None\n",
        "\n",
        "# def extract_metadata(prompt, query):\n",
        "#     \"\"\"\n",
        "#     Extracts metadata from a support ticket using the OpenAI model and returns the result in JSON format.\n",
        "#     Args:\n",
        "#         prompt (str): The metadata extraction prompt for the model.\n",
        "#         query (str): The support ticket text to extract metadata from.\n",
        "#     Returns:\n",
        "#         dict: A dictionary containing the extracted metadata, or None if extraction fails.\n",
        "#     \"\"\"\n",
        "#     try:\n",
        "#         response_text = query_openai(prompt, query)\n",
        "#         # Attempt to parse the response text as JSON\n",
        "#         metadata_result = json.loads(response_text)\n",
        "#         return metadata_result\n",
        "#     except json.JSONDecodeError as e:\n",
        "#         st.error(f\"Error decoding JSON from OpenAI response: {e}\")\n",
        "#         st.text(f\"Raw OpenAI response: {response_text}\")\n",
        "#         return None\n",
        "#     except Exception as e:\n",
        "#         st.error(f\"An unexpected error occurred during metadata extraction: {e}\")\n",
        "#         return None\n",
        "\n",
        "# def predict_priority(prompt, query, problem_type, user_impact):\n",
        "#     \"\"\"\n",
        "#     Predicts the priority of a support ticket using the OpenAI model and returns the result in JSON format.\n",
        "#     Args:\n",
        "#         prompt (str): The priority prediction prompt for the model.\n",
        "#         query (str): The support ticket text to predict the priority for.\n",
        "#         problem_type (str): The extracted problem type.\n",
        "#         user_impact (str): The extracted user impact.\n",
        "#     Returns:\n",
        "#         dict: A dictionary containing the predicted priority, or None if prediction fails.\n",
        "#     \"\"\"\n",
        "#     try:\n",
        "#         # Include problem_type and user_impact in the query sent to the model\n",
        "#         full_query = f\"\"\"\n",
        "#         Support Ticket: {query}\n",
        "#         Problem Type: {problem_type}\n",
        "#         User Impact: {user_impact}\n",
        "#         Based on the support ticket, problem type, and user impact, predict the priority: Low, Medium, High, or Urgent.\n",
        "#         Return only a structured JSON output in the following format:\n",
        "#         {{\"priority\": \"priority_prediction\"}}\n",
        "#         \"\"\"\n",
        "#         response_text = query_openai(prompt, full_query)\n",
        "#         priority_result = json.loads(response_text)\n",
        "#         return priority_result\n",
        "#     except json.JSONDecodeError as e:\n",
        "#         st.error(f\"Error decoding JSON from OpenAI response: {e}\")\n",
        "#         st.text(f\"Raw OpenAI response: {response_text}\")\n",
        "#         return None\n",
        "#     except Exception as e:\n",
        "#         st.error(f\"An unexpected error occurred during priority prediction: {e}\")\n",
        "#         return None\n",
        "\n",
        "# def generate_response(response_prompt, query, category, metadata_tags, priority):\n",
        "#     \"\"\"\n",
        "#     Generates a draft response for a support ticket using the OpenAI model.\n",
        "#     Args:\n",
        "#         response_prompt (str): The prompt for generating the response.\n",
        "#         query (str): The original support ticket text.\n",
        "#         category (str): The predicted category of the ticket.\n",
        "#         metadata_tags (dict): The extracted metadata tags (Device, Problem Type, User Impact).\n",
        "#         priority (str): The predicted priority of the ticket.\n",
        "#     Returns:\n",
        "#         str: The generated response text, or None if response generation fails.\n",
        "#     \"\"\"\n",
        "#     # Combine the inputs into a single message for the model\n",
        "#     user_message = f\"\"\"\n",
        "#     Support Ticket: {query}\n",
        "#     Category: {category}\n",
        "#     Metadata Tags: {metadata_tags}\n",
        "#     Priority: {priority}\n",
        "#     \"\"\"\n",
        "\n",
        "#     try:\n",
        "#         # Pass the combined message to the query_openai function\n",
        "#         response_text = query_openai(response_prompt, user_message)\n",
        "#         return response_text\n",
        "#     except Exception as e:\n",
        "#         st.error(f\"An unexpected error occurred during response generation: {e}\")\n",
        "#         return None\n",
        "\n",
        "# # Define the prompts\n",
        "# classification_prompt = \"\"\"\n",
        "#  You are a technical assistant. Classify the support ticket based on the Support Ticket Text presented in the input into the following categories and not any other.\n",
        "#       - Technical issues\n",
        "#       - Hardware issues\n",
        "#       - Data recovery\n",
        "#   Return only a structured JSON output in the following format:\n",
        "#   {\"Category\": \"category_prediction\"}\n",
        "# \"\"\"\n",
        "\n",
        "# metadata_prompt = f\"\"\"\n",
        "# You are an intelligent assistant that extracts structured metadata from technical support queries.\n",
        "# Analyze the query and extract the following information:\n",
        "\n",
        "# * Device (e.g., Laptop, Phone, Router, etc.)\n",
        "# * Problem Type (e.g., Not Turning On, Lost Internet, Deleted Files)\n",
        "# * User Impact - Estimate based on how severely the issue affects the user's ability to continue working or using the device:\n",
        "\n",
        "#     - * Major: The user cannot proceed with work at all.\n",
        "#     - * Moderate: The user is impacted but may have a workaround.\n",
        "#     - * Minor: The issue is present but does not significantly hinder usage.\n",
        "\n",
        "# Use the following examples as guidance.\n",
        "\n",
        "# Query Text: My phone battery is draining rapidly even on battery saver mode. I barely use it and it drops 50% in a few hours.\n",
        "# Output: {\"Device\": \"Phone\", \"Problem Type\": \"Battery Draining\", \"User Impact\": \"Minor\"}\n",
        "\n",
        "# Query Text: I accidentally deleted a folder containing all project files. Please help me recover it.\n",
        "# Output: {\"Device\": \"Laptop\", \"Problem Type\": \"Deleted Files\", \"User Impact\": \"Major\"}\n",
        "\n",
        "# Query Text: My router is not working.\n",
        "# Output: {\"Device\": \"Router\", \"Problem Type\": \"Lost Internet\", \"User Impact\": \"Moderate\"}\n",
        "\n",
        "# Return the final output only in a valid JSON format without any additional explanation.\n",
        "# \"\"\"\n",
        "\n",
        "# priority_prompt =\"\"\"\n",
        "# You are an intelligent assistant that determines the priority level of a support ticket.\n",
        "# For any given ticket, follow this step-by-step reasoning process to assign the correct priority level: Low, Medium, High.\n",
        "# Step-by-step Evaluation:\n",
        "# Is the device or service completely unusable?\n",
        "# Is the issue blocking critical or time-sensitive work?\n",
        "# Is there a specific deadline or urgency mentioned by the user?\n",
        "# Does the user mention partial functionality or ongoing work?\n",
        "# Is the tone or language expressing frustration or emergency?\n",
        "# After evaluating each step, decide the most appropriate priority level based on the impact and urgency.\n",
        "# Finally, return only the structured output in valid JSON format, like this:\n",
        "# {\"priority\": \"High\"}\n",
        "# Do not include your reasoning in the output — just the JSON.\n",
        "# \"\"\"\n",
        "\n",
        "# response_prompt = \"\"\"\n",
        "# You are provided with a support ticket's text along with its Category, Tags, and assigned Priority level.\n",
        "\n",
        "# Follow these steps before generating your final response:\n",
        "\n",
        "# 1. Analyze the ticket text to understand the customer's sentiment and main concern.\n",
        "# 2. Identify the issue type using the provided Category and Tags.\n",
        "# 3. Determine the appropriate ETA based on the Priority level.\n",
        "# 4. Compose a short, empathetic response that reassures the customer, acknowledges their concern, and includes the ETA.\n",
        "\n",
        "# Ensure the final response:\n",
        "\n",
        "# 1. Is under 50 words\n",
        "# 2. Has a polite and empathetic tone\n",
        "# 3. Addresses the issue clearly\n",
        "\n",
        "# Return only the final response to the customer. Do not include your reasoning steps in the output.\n",
        "# \"\"\"\n",
        "\n",
        "\n",
        "# # Streamlit App\n",
        "# st.title(\"Support Ticket Categorization System\")\n",
        "\n",
        "# st.write(\"Enter the support ticket text below:\")\n",
        "\n",
        "# support_ticket_input = st.text_area(\"Support Ticket Text\", height=200)\n",
        "\n",
        "# if st.button(\"Process Ticket\"):\n",
        "#     if support_ticket_input:\n",
        "#         st.write(\"Processing...\")\n",
        "\n",
        "#         # Categorization\n",
        "#         category_result = classify_ticket(classification_prompt, support_ticket_input)\n",
        "#         category = category_result.get('Category') if category_result else \"N/A\"\n",
        "#         st.subheader(\"Category:\")\n",
        "#         st.write(category)\n",
        "\n",
        "#         # Metadata Extraction\n",
        "#         metadata_result = extract_metadata(metadata_prompt, support_ticket_input)\n",
        "#         device = metadata_result.get('Device') if metadata_result else \"N/A\"\n",
        "#         problem_type = metadata_result.get('Problem Type') if metadata_result else \"N/A\"\n",
        "#         user_impact = metadata_result.get('User Impact') if metadata_result else \"N/A\"\n",
        "\n",
        "#         st.subheader(\"Metadata:\")\n",
        "#         st.write(f\"Device: {device}\")\n",
        "#         st.write(f\"Problem Type: {problem_type}\")\n",
        "#         st.write(f\"User Impact: {user_impact}\")\n",
        "\n",
        "#         # Priority Prediction\n",
        "#         priority_result = predict_priority(priority_prompt, support_ticket_input, problem_type, user_impact)\n",
        "#         priority = priority_result.get('priority') if priority_result else \"N/A\"\n",
        "#         st.subheader(\"Priority:\")\n",
        "#         st.write(priority)\n",
        "\n",
        "#         # Draft Response Generation\n",
        "#         draft_response = generate_response(response_prompt, support_ticket_input, category, metadata_result, priority)\n",
        "#         st.subheader(\"Draft Response:\")\n",
        "#         st.write(draft_response)\n",
        "\n",
        "\n",
        "#     else:\n",
        "#         st.warning(\"Please enter support ticket text to process.\")\n"
      ],
      "metadata": {
        "id": "m_FsPjxIIZKa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Docker File"
      ],
      "metadata": {
        "id": "17VcIdIcDadL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile Dockerfile\n",
        "\n",
        "# FROM python:3.9-slim\n",
        "\n",
        "# WORKDIR /app\n",
        "\n",
        "# RUN apt-get update && apt-get install -y \\\n",
        "#     build-essential \\\n",
        "#     curl \\\n",
        "#     software-properties-common \\\n",
        "#     git \\\n",
        "#     && rm -rf /var/lib/apt/lists/*\n",
        "\n",
        "# COPY requirements.txt ./\n",
        "# COPY app.py ./\n",
        "# COPY src/ ./src/\n",
        "\n",
        "# RUN pip3 install -r requirements.txt\n",
        "\n",
        "# EXPOSE 8501\n",
        "\n",
        "# HEALTHCHECK CMD curl --fail http://localhost:8501/_stcore/health\n",
        "\n",
        "# ENTRYPOINT [\"streamlit\", \"run\", \"app.py\", \"--server.port=8501\", \"--server.address=0.0.0.0\"]\n"
      ],
      "metadata": {
        "id": "ASfiJU8kDXsP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### requirements.txt"
      ],
      "metadata": {
        "id": "5DSuuldPKKId"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %%writefile requirements.txt\n",
        "# altair==5.5.0\n",
        "# pandas==2.2.2\n",
        "# streamlit==1.47.1\n",
        "# openai==1.97.1"
      ],
      "metadata": {
        "id": "R4lT85myKON8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hugging Face Setup"
      ],
      "metadata": {
        "id": "LgOKkAclIi46"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Login to Hugging Face"
      ],
      "metadata": {
        "id": "7nTsv5ZqoH-z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Go to [Hugging face](https://huggingface.co) and sign up or log in to your account."
      ],
      "metadata": {
        "id": "um5a84WIoP2j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Create a New Space"
      ],
      "metadata": {
        "id": "MCzdrLYgoYsG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "   * Navigate to [Hugging face Spaces ](https://huggingface.co/spaces).\n",
        "   * Click **Create New Space**.\n",
        "   * Fill in:\n",
        "\n",
        "     * Name for your Space.\n",
        "     * Space SDK: Select **Docker**.\n",
        "     * Choose a Docker template: Select **Streamlit**\n",
        "     * Visibility: Choose *Public* or *Private*.\n",
        "   * Click **Create Space**."
      ],
      "metadata": {
        "id": "iReg7welocEh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Setup OpenAI tokens\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "N_GU8gcMJ5e1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Open your Hugging Face Space\n",
        "\n",
        "- Click on the **“Settings”** tab at the top of the Space.\n",
        "\n",
        "- Scroll down to the **“Repository secrets”** section.\n",
        "\n",
        "- Click **“Add a new secret”**.\n",
        "\n",
        "- In the **Name** field, type token name\n",
        "\n",
        "- In the **Secret** field, paste your secret key\n",
        "\n",
        "- Click **“Add secret”** to save it.\n"
      ],
      "metadata": {
        "id": "3p8iaRcqJwPk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Upload Your Files\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mrEhsirkos1w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "   * In the new Space, click the **Files** tab.\n",
        "   * Delete existing requirements.txt , DockerFile\n",
        "   * Click Contribute then Upload files and add:\n",
        "\n",
        "     * `app.py`\n",
        "     * `requirements.txt`\n",
        "     * `DockerFile`\n",
        "   * Commit the upload."
      ],
      "metadata": {
        "id": "8f2e8YFbo2Jg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Build and Launch\n",
        "\n"
      ],
      "metadata": {
        "id": "mMJf9s2WpMoi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "   * Hugging Face will automatically detect the `Dockerfile` and start building the container.\n",
        "   * Wait a few minutes for the build to complete and the app to go live."
      ],
      "metadata": {
        "id": "Z7MBsOPppdvk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Access and Test"
      ],
      "metadata": {
        "id": "nIHFm30QpluH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Go to the App tab or the Space URL to view and test your running Streamlit app.\n",
        "\n"
      ],
      "metadata": {
        "id": "d_hHgiFlpoN0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion"
      ],
      "metadata": {
        "id": "v_OxiKnrjQ1M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The primary objective here was to demonstrate how we can convert unstructured support ticket text into structured information using large language models and prompt engineering.  \n",
        "\n",
        "By classifying tickets into categories, assigning priorities, and generating structured outputs in JSON format, we’ve created a scalable and reusable system that can significantly streamline customer support operations.\n",
        "\n",
        "\n",
        "**Business Impact**\n",
        "\n",
        "- **Improved Efficiency:** Automating ticket classification and prioritization helps support teams triage issues faster, reducing resolution time.\n",
        "- **Better Resource Allocation:** Tickets can be routed based on urgency and category, ensuring critical issues are addressed first.\n",
        "- **Data-Driven Insights:** Structured data enables better tracking, reporting, and trend analysis.\n",
        "\n",
        "This approach serves as a strong foundation for building intelligent customer support systems that are adaptable, efficient, and business-aligned.\n",
        "\n",
        "**Improvement Areas**\n",
        "\n",
        "- The current system lacks access to the latest company policies and procedural updates, making responses generic or outdated. RAG addresses this by retrieving the most relevant, up-to-date internal documents to generate accurate and context-aware replies.\n"
      ],
      "metadata": {
        "id": "XcP6LhUoCd0Z"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybRlzaIhWaM9"
      },
      "source": [
        "<font size=6 color='#4682B4'>Power Ahead</font>\n",
        "___"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "5Bxnkoxf270y",
        "AAaZfw3ae5hc",
        "UfqBaEwD3FNh",
        "RO7H34JN34vK",
        "17VcIdIcDadL",
        "5DSuuldPKKId",
        "LgOKkAclIi46",
        "7nTsv5ZqoH-z",
        "MCzdrLYgoYsG",
        "N_GU8gcMJ5e1",
        "mrEhsirkos1w",
        "mMJf9s2WpMoi",
        "nIHFm30QpluH"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}